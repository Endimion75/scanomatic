In here are lagacy stuff that has been removed

"""
FROM 
"""

#!/usr/bin/env python
"""
The module rewrites logfiles created as projects are running.
"""
__author__ = "Martin Zackrisson"
__copyright__ = "Swedish copyright laws apply"
__credits__ = ["Martin Zackrisson"]
__license__ = "GPL v3.0"
__version__ = "0.997"
__maintainer__ = "Martin Zackrisson"
__email__ = "martin.zackrisson@gu.se"
__status__ = "Development"


#
# DEPENDENCIES
#

import logging, uuid, os

#
# GLOBALS
#

META_DATA = {'Start Time': 0, 'Prefix': 'unknown', 'Interval': 20.0, 
   'Description': 'Automatic placeholder description',
   'UUID': uuid.uuid1().get_urn().split(":")[-1],'Measures':0,'Fixture': 'fixture_a',
   'Pinning Matrices': None, 'Manual Gridding': None}

#
# FUNCTIONS
#

def rewrite_meta_row(path, header_entries):

    try:
        fs = open(path, 'r')
    except:
        logging.error("Could not open '{0}'".format(path))
        return False


    lines = []
    i = 0
    for l in fs:
        if i == 0:
            l_dict = eval(l)
            try:
                l_keys_in_meta = [(k in META_DATA.keys()) for k in l_dict.keys()]
            except:
                l_keys_in_meta = [False]

            if len(l_keys_in_meta) > sum(l_keys_in_meta):
                data = l_dict
            else:
                data = META_DATA

            for h in header_entries.keys():
                data[h] = header_entries[h]
             
            if len(l_keys_in_meta) > sum(l_keys_in_meta):
                lines = [str(data)+"\n", l]
            else:
                lines = [str(data)+"\n"]
        else:
            lines.append(l)
        i += 1
    fs.close()

    fs = open(path, 'w')
    for l in lines:
        fs.write(l)
    fs.close()
    return True

def rewrite(path, rewrite_entries):

    try:
        fs = open(path, 'r')
    except:
        logging.error("Could not open '{0}'".format(path))
        return False


    lines = []

    i = 0
    for l in fs:
        if i in rewrite_entries.keys():
            lines.append(str(rewrite_entries[i]['data']))
            if rewrite_entries[i]['mode'] == 'a':
                lines.append(l)
        else:
            lines.append(l)
        i += 1
    fs.close()

    fs = open(path, 'w')

    for l in lines:
        fs.write(l)

    fs.close()
    return True

def create_place_holder_meta_info(path = None):

    data = META_DATA

    if path:

        try:

            fs = open(path, 'r')
            file_exists = True

        except: 

            logging.warning("The file at '{0}' could not been opened".format(path))
            file_exists = False

        if file_exists:

            lowest_time = None
            n_images = 0
            prefix = None

            for line in fs:
                try:
                    l = eval(line)
                except:
                    l = {}

                try:
                    'mark_X' in l.keys()
                    good_line = True
                except:
                    loggin.warning("The file '{0}' contains unexpected line '{1}'"\
                        .format(path, line))
                    good_line = False

                if good_line: 
                    if 'Time' in l.keys():
                        if lowest_time is None or l['Time'] < lowest_time:
                            lowest_time = l['Time']

                    if prefix is None and 'File' in l.keys():
                        try:
                            prefix = l['File'].split(os.sep)[-1][:-10]
                        except:
                            logging.warning('Problem guessing the original prefix')

                    if 'mark_Y' in l.keys():

                        n_images += 1
            fs.close()

            if prefix is not None:
                data['Prefix'] = prefix
            if lowest_time is not None:
                data['Start Time'] = lowest_time
            data['Measures'] = n_images

    return data


"""
FROM resource_log_reader
"""

#!/usr/bin/env python
"""This script produces analysis of inter-scan noise within a project."""

__author__ = "Martin Zackrisson"
__copyright__ = "Swedish copyright laws apply"
__credits__ = ["Martin Zackrisson"]
__license__ = "GPL v3.0"
__version__ = "0.997"
__maintainer__ = "Martin Zackrisson"
__email__ = "martin.zackrisson@gu.se"
__status__ = "Development"

#
# DEPENDENCIES
#

import matplotlib.pyplot as plt
import numpy as np
import sys, os
import types

#
# SCANNOMATIC DEPENDENCIES
#

import resource_image_reject as reject_script

#
# GLOBALS
#

_histograms = []

#
# EXCEPTIONS
#

class FileError(Exception): pass

#
# FUNCTIONS
#


def get_all_data(f):


    try:

        fs = open(f, 'r')

    except:

        raise FileError("File '{0}' doesn't exist".format(f))

    lines = list()

    for line in fs.readlines():

        try:
            l = eval(line)

            if type(l) == types.DictType:

                lines.append(l)
        except:
            pass
            #print "Lost line \n{0}\n".format(line)

    fs.close()

    return lines


def get_im_data(f, im_path, only_file_name=True, im_path_prefix='File'):

    d = get_all_data(f)

    if only_file_name:

        im_path = im_path.split(os.sep)[-1]

    im_data = [l for l in d if im_path_prefix in l.keys() and im_path in l[im_path_prefix]]

    if len(im_data) > 0:

        return im_data[0]

    else:

        return None


def count_histograms():
    return len(_histograms)


def load_data(path):

    global _histograms

    _histograms = []
    try:
        fs = open(path,'r')           
    except:
        print "Error, the file '" + str(sys.argv[1]) + "' does not exist."

    while True:
        line = fs.readline()
        if not line:
            break
        put_data_where_it_belongs(line)

    fs.close()

def put_data_where_it_belongs(data):

    global _histograms

    try:
        data = eval(str(data).strip())
    except:
        print "Non-interpretable line:", data

    if type(data) == types.DictType:
        if "Histogram" in data.keys():
            _histograms.append((str(data['File']), data['Histogram']))

def display_histograms(files="", draw_plot=True, mark_rejected=True, threshold=1.0, threshold_less_than=True, log_file=None, manual_value=None, max_value=255, save_path=None):

    global _histograms

    plot_data = None
    plot_indices = []

    if type(files) != types.ListType:
        files = [files]


    for f in files:
        if f == None:
            f = ""
        
        for k in xrange(len(_histograms)):
            if str(f) in _histograms[k][0]:
                if plot_data == None:
                    plot_data = np.array(_histograms[k][1])
                else:
                    plot_data = np.vstack((plot_data, np.array(_histograms[k][1])))
                plot_indices.append(k)

    #print plot_data.shape
    #print len(_histograms), plot_indices
    #print _histograms

    if mark_rejected:
            rejections = reject_script.evaluate_images(log_file=log_file, manual_value=manual_value, threshold=threshold, threshold_less_than=threshold_less_than, max_value=max_value)

    if plot_data != None and draw_plot:
        plt.plot(plot_data.T)
        plt.ylabel("count")
        plt.xlabel("inv pixel value")
        plt.legend(plot_keys)
    else:
        plt.imshow(plot_data)
        plt.ylabel("image number")
        plt.xlabel("inv pixel value")
        if mark_rejected and True == False: #HACK Cause it's ugly
            for r in rejections:
                if r['Source Index'] in plot_indices:
                    plt.plot(np.array(xrange(max_value+1)), r['Source Index']*np.ones(max_value+1, int), 'w-', alpha=190, linewidth=4)
        if save_path:
                plt.savefig(save_path)
                plt.clf()
    if mark_rejected:
            return rejections
    else:
            return plot_data

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] != "-h":

            load_data(sys.argv[-1])
            A = None
            do_histograms = True
            threshold = None
            manual_value  = None
            max_value = 255
            threshold_less_than = True

            if "-t" in sys.argv:
                for i in xrange(len(sys.argv)):
                    if sys.argv[i] == "-t":
                        try:
                            threshold = float(sys.argv[i+1])
                        except:
                            pass

            if "-v" in sys.argv:
                for i in xrange(len(sys.argv)):
                    if sys.argv[i] == "-v":
                        try:
                            manual_value = float(sys.argv[i+1])
                        except:
                            pass

            if "-m" in sys.argv:
                for i in xrange(len(sys.argv)):
                    if sys.argv[i] == "-m":
                        try:
                            max_value = int(sys.argv[i+1])
                        except:
                            pass

            if "-g" in sys.argv:
                threshold_less_than = False
                                    
            if "-i" in sys.argv:
                do_histograms = False

            if "-p" in sys.argv:
                start_slice = None
                stop_slice = None
                for i, a in enumerate(sys.argv[:-1]):
                    if not start_slice:
                        if a == "-p" and i+1 < len(sys.argv):
                            start_slice = i+1
                    elif not stop_slice:
                        if str(a)[0] == "-":
                            stop_slice = i+1

                if not stop_slice:
                    stop_slice = i+1
                if start_slice:
                    A = display_histograms(sys.argv[start_slice:stop_slice], do_histograms, log_file=sys.argv[-1], max_value=max_value, manual_value=manual_value, threshold=threshold, threshold_less_than=threshold_less_than)

            if "-i" in sys.argv:
                if A == None:
                    A = display_histograms("",do_histograms, log_file=sys.argv[-1], max_value=max_value, manual_value=manual_value, threshold=threshold, threshold_less_than=threshold_less_than)


            plt.show()

    else:
            print "You need to run this script with the file you want to convert as argument"
            print "COMMAND:",sys.argv[0],"[OPTIONS] [LOG-FILE]"
            print "\n\nOPTIONS:"
            print "-f [PATTERNS]\t\tPlots a histogram containing plots for all files in the log-file that matches a pattern"
            print "-i\t\t\tUses imshow to display a composite heatmap instead of histograms"
            print "-t x.xx\t\tSet exclusion threshold level (default = 1.0)"
            print "-g\t\t\tInvert threshold logic (default logic is to mark those less than threshold)"
            print "-m xxx\t\tSet pixel max value (default 255)"
            print "-v x.xx\t\tSet default target value for exclusion script manually (default is using median of medians)"
            sys.exit(0)


"""
FROM resource_image
CLASS: Analyse_Grayscale
"""

    def get_grayscale(self, image=None, scale_factor=1.0, dpi=600):

        if image != None:

            self._img = image

        if self._img is None or sum(self._img.shape) == 0:

            return None

        #DEBUG PLOT
        #plt.imshow(self._img)
        #plt.show()
        #DEBUG PLOT END

        if scale_factor == 1 and dpi != 600:

            scale_factor = 600.0 / dpi

        logging.debug("GRAYSCALE ANALYSIS: Of images "
            "{0} at dpi={1} and scale_factor={2}".format(
            self._img.shape, dpi, scale_factor))

        A = (self._img[self._img.shape[0] / 2:, :] < (256 * 1 / 4)).astype(int)
        #B = (self._img[:self._img.shape[0]/2,:] > (256*2/4)).astype(int)
        #DEBUG PLOT
        #plt.subplot(121)
        #plt.imshow(A)
        #plt.subplot(122)
        #plt.imshow(B)
        #plt.show()
        #DEBUG PLOT END

        rect = [[0, 0], [self._img.shape[0], self._img.shape[1]]]
        orth_diff = -1

        kern = np.asarray([-1, 0, 1])
        Aorth = A.mean(0)
        Aorth_edge = abs(fftconvolve(kern, Aorth, 'same'))
        Aorth_edge_threshold = 0.2  # Aorth_edge.max() * 0.70
        Aorth_signals = Aorth_edge > Aorth_edge_threshold
        Aorth_positions = np.where(Aorth_signals)

        if len(Aorth_positions[0]) > 1:

            Aorth_pos_diff = abs(1 - ((Aorth_positions[0][1:] - \
                Aorth_positions[0][:-1]) / float(self._grayscale_width)))
            rect[0][1] = Aorth_positions[0][Aorth_pos_diff.argmin()]
            rect[1][1] = Aorth_positions[0][Aorth_pos_diff.argmin() + 1]
            orth_diff = rect[1][1] - rect[0][1]

        if orth_diff == -1:

            #Orthagonal trim second try
            firstPass = True
            in_strip = False
            threshold = 0.15
            threshold2 = 0.3

            for i, orth in enumerate(A.mean(0)):

                if firstPass:

                    firstPass = False

                else:

                    if abs(old_orth - orth) > threshold:

                        if in_strip == False:

                            if orth > threshold2:

                                rect[0][1] = i
                                in_strip = True

                        else:

                            rect[1][1] = i
                            break

                old_orth = orth

            orth_diff = rect[1][1] - rect[0][1]

        #safty margin
        min_orths = 30 / scale_factor

        if orth_diff > min_orths:

            rect[0][1] += (orth_diff - min_orths) / 2
            rect[1][1] -= (orth_diff - min_orths) / 2

        self._mid_orth_strip = rect[0][1] + (rect[1][1] - rect[0][1]) / 2

        #Paralell trim - right
        box_need = self._grayscale_lower_than_half_width / \
                                                scale_factor

        i = (rect[1][0] - rect[0][0]) / 2.0

        #DEBUG PLOT
        #plt.imshow(self._img[rect[0][0]:rect[1][0],rect[0][1]:rect[1][1]])
        #plt.show()
        #END DEBUG PLOT

        strip_values = self._img[rect[0][0]: rect[1][0],
                            rect[0][1]: rect[1][1]].mean(1)

        A2 = strip_values > 125

        A2_where = np.where(A2 == True)[0]

        if A2_where.size > 1:

            A2_rights = np.append(A2_where[0], A2_where[1:] - A2_where[:-1])
            edges = np.argsort(A2_rights)
            edge_pos = -1

            while A2_rights[edges[edge_pos]] > box_need:

                right_edge = edges[edge_pos]

                if A2_rights[right_edge] > box_need and \
                        A2_rights[:right_edge + 1].sum() > i:

                    rect[1][0] = A2_rights[:right_edge + 1].sum() + \
                        self._grayscale_length / 2.0

                    if rect[0][0] < 0:

                        logging.warning('GRAYSCALE, Overshot upper bound ' +
                                    'setting. Compensating')

                        rect[0][0] = self._img.shape[0]

                    break

                else:

                    edge_pos -= 1

                    if abs(edge_pos) == edges.size:

                        break

        box_need = self._grayscale_higher_than_half_width / \
                                 scale_factor

        A2 = A2 == False
        A2_where = np.where(A2 == True)[0]

        if A2_where.size > 1:

            A2_lefts = np.append(A2_where[0], A2_where[1:] - A2_where[:-1])
            edges = np.argsort(A2_lefts)
            edge_pos = -1

            while A2_lefts[edges[edge_pos]] > box_need:

                left_edge = edges[edge_pos]

                if A2_lefts[left_edge] > box_need and \
                        A2_lefts[:left_edge + 1].sum() < i:

                    rect[0][0] = A2_lefts[:left_edge].sum() - \
                        self._grayscale_length / 2.0

                    if rect[0][0] < 0:

                        logging.warning('GRAYSCALE, Overshot lower bound' +
                                ' setting. Compensating')

                        rect[0][0] = 0

                    break

                else:

                    edge_pos -= 1

                    if abs(edge_pos) == edges.size:

                        break

        ###DEBUG CUT SECTION
        #plt.clf()
        #plt.imshow(self._img[rect[0][0]:rect[1][0],rect[0][1]:rect[1][1]])
        #plt.show()
        ###DEBUG END

        strip_values = self._img[rect[0][0]: rect[1][0],
                        rect[0][1]: rect[1][1]].mean(1)

        threshold = 1.2
        kernel = [-1, 1]  # old [-1,2,-1]

        up_spikes = np.abs(np.convolve(strip_values, kernel,
                "same")) > threshold

        up_spikes = r_signal.get_center_of_spikes(up_spikes)

        best_spikes = r_signal.get_best_spikes(up_spikes,
            self._grayscale_length / scale_factor,
            tollerance=0.05,
            require_both_sides=False)

        frequency = r_signal.get_perfect_frequency2(best_spikes,
            self._grayscale_length / scale_factor)

        offset = r_signal.get_best_offset(
            self._grayscale_sections,
            best_spikes, frequency=frequency)

        signal = r_signal.get_true_signal(self._img.shape[0],
            self._grayscale_sections,
            up_spikes, frequency=frequency,
            offset=offset)

        if signal is None:

            logging.warning(("GRAYSCALE, no signal detected for f={0} and"
                " offset={1} in best_spikes={2} from spikes={3}").format(
                frequency, offset, best_spikes, up_spikes))

            return None, None

        safety_buffer = 0.2

        if signal[0] - frequency + frequency * safety_buffer < 0:

            logging.warning("GRAYSCALE, the signal got adjusted one interval"
                " due to lower bound overshoot")

            signal += frequency

        if signal[-2] - frequency * safety_buffer > strip_values.size:

            logging.warning("GRAYSCALE, the signal got adjusted one interval"
                " due to upper bound overshoot")

            signal -= frequency

        safety_coeff = 0.5
        gray_scale = []
        gray_scale_pos = []

        self.ortho_half_height = self._grayscale_width / \
            2.0 * safety_coeff

        for pos in xrange(signal.size - 1):

            gray_scale_pos.append(signal[pos] - 0.5 * frequency + rect[0][0])
            gray_scale.append(
                self._img[gray_scale_pos[-1] - 0.5 * frequency * safety_coeff:
                    gray_scale_pos[-1] + 0.5 * frequency * safety_coeff,
                    self._mid_orth_strip - self.ortho_half_height:
                    self._mid_orth_strip + self.ortho_half_height].mean())

        #from matplotlib import pyplot as plt
        #print offset, frequency
        #strip_values = self._img[:,rect[0][1]:rect[1][1]].mean(1)
        #plt.plot(strip_values)
        #plt.plot(np.arange(up_spikes.size)+rect[0][0],up_spikes*
            #strip_values.max())
        #plt.plot(np.arange(best_spikes.size)+rect[0][0],
            #best_spikes*strip_values.max()*0.5)
        #plt.plot(
            #np.arange(self._grayscale_sections+2)
            #*frequency+offset+rect[0][0],
            #np.ones(self._grayscale_sections+2)
            #*0.5*strip_values.max(), '*')
        #plt.plot(np.asarray(gray_scale_pos), gray_scale, 'o')
        #plt.plot((rect[0][0], rect[1][0]),np.ones(2)*0.75*
            #strip_values.max(),'*')
        #plt.show()

        #DEBUG PLOT
        #plt.imshow(self._img.T, cmap=plt.cm.gray)
        #plt.plot(gray_scale_pos, np.ones(len(gray_scale_pos))
            #*self._mid_orth_strip -\
             #self.ortho_half_height ,
            #'r-' )
        #plt.plot(gray_scale_pos, np.ones(len(gray_scale_pos))
            #*self._mid_orth_strip +\
             #self.ortho_half_height,
            #'r-' )
        #plt.plot(gray_scale_pos, np.ones(len(gray_scale_pos))
            #*self._mid_orth_strip ,
            #'ro' )
        #plt.plot(gray_scale_pos, gray_scale, 'b--')
        #plt.show()
        #END DEBUG PLOT

        self._gray_scale_pos = gray_scale_pos
        self._gray_scale = gray_scale

        return gray_scale_pos, gray_scale

    def get_old_grayscale(self, image=None, scale_factor=1.0, dpi=600):

        if image is not None:

            self._img = image

        if self._img is None:

            return None

        A = (self._img < (self._img.max() / 4)).astype(int)

        rect = [[0, 0], [self._img.shape[0], self._img.shape[1]]]
        orth_diff = -1

        kern = np.asarray([-1, 0, 1])
        Aorth = A.mean(0)
        Aorth_edge = abs(fftconvolve(kern, Aorth, 'same'))
        Aorth_edge_threshold = 0.2  # Aorth_edge.max() * 0.70
        Aorth_signals = Aorth_edge > Aorth_edge_threshold
        Aorth_positions = np.where(Aorth_signals)

        if len(Aorth_positions[0]) > 1:

            Aorth_pos_diff = abs(1 - ((Aorth_positions[0][1:] -
                Aorth_positions[0][:-1]) / float(self._grayscale_width)))

            rect[0][1] = Aorth_positions[0][Aorth_pos_diff.argmin()]
            rect[1][1] = Aorth_positions[0][Aorth_pos_diff.argmin() + 1]
            orth_diff = rect[1][1] - rect[0][1]

            ### DEBUG CONVOLVE FINDING
            #print Aorth_pos_diff
            #plt.plot(np.asarray([Aorth_positions[0][Aorth_pos_diff.argmin()],
            #    Aorth_positions[0][Aorth_pos_diff.argmin()+1]]),
                    #np.ones(2)*0.75,
            #    lw=5)
        #plt.plot(np.arange(len(Aorth)),Aorth)
        #plt.plot(np.arange(len(Aorth)), Aorth_edge)
        #plt.plot(np.arange(len(Aorth)), Aorth_signals)
        #plt.show()
        ###DEBUG END

        if orth_diff == -1:
            #Orthagonal trim second try
            firstPass = True
            in_strip = False
            threshold = 0.15
            threshold2 = 0.3

            for i, orth in enumerate(A.mean(0)):

                if firstPass:

                    firstPass = False

                else:

                    if abs(old_orth - orth) > threshold:

                        if in_strip == False:

                            if orth > threshold2:

                                rect[0][1] = i
                                in_strip = True

                        else:

                            rect[1][1] = i
                            break

                old_orth = orth

            orth_diff = rect[1][1] - rect[0][1]

        #safty margin

        min_orths = 30 / scale_factor

        if orth_diff > min_orths:

            rect[0][1] += (orth_diff - min_orths) / 2
            rect[1][1] -= (orth_diff - min_orths) / 2

        self._mid_orth_strip = (rect[0][1] + rect[1][1]) / 2.0

        ###DEBUG UNCUT SECTION
        #plt.clf()
        #plt.imshow(A)
        #plt.show()
        #plt.savefig('gs_test1.png')
        ###DEBUG END

        #Paralell trim
        i = (rect[1][0] - rect[0][0]) / 2
        box_needed = 120 / scale_factor
        boxsize = 0
        transition = 0

        A2 = (self._img[rect[0][0]: rect[1][0],
            rect[0][1]: rect[1][1]] <
            np.median(self._img)).astype(int).mean(1)

        ###DEBUG CUT SECTION
        #plt.clf()
        #plt.imshow(self._img[rect[0][0]:rect[1][0],rect[0][1]:rect[1][1]])
        #plt.show()
        ###DEBUG END

        while i > 0:

            if A2[i] > 0.2:

                if boxsize > box_needed and A2[i] > 0.3:

                    break

                elif transition < 2 / scale_factor:

                    transition += 1

                else:

                    boxsize = 0
                    transition = 0

            else:

                boxsize += 1

            i -= 1
            #print i, boxsize, A2[i]

        rect[0][0] = i  # Some margin?
        #if rect[0][0] < 0:
        #    rect[0][0] = 0

        #plt.clf()
        #plt.imshow(self._img[rect[0][0]:rect[1][0],rect[0][1]:rect[1][1]].T)
        #plt.savefig('gs_test2.png')

        #plt.clf()
        #plt.plot(A2)
        #plt.savefig('gs_test_2b.png')
        #print rect
        #np.savetxt('dev.txt', self._img[rect[0][0]:rect[1][0],
            #rect[0][1]:rect[1][1]])

        #producing the gray_scale values
        #np.savetxt('grayscalearea.array', self._img[rect[0][0]:rect[1][0],
            #rect[0][1]:rect[1][1]])

        strip_values = self._img[rect[0][0]: rect[1][0],
                            rect[0][1]: rect[1][1]].mean(1)

        target_count = 23
        target_length = 29 / scale_factor
        threshold = 1.2
        previous_spike = 0
        low_length_tolerance = target_length - 5 / scale_factor
        high_length_tolerance = target_length + 5 / scale_factor
        kernel = [1, -1]  # old [-1,2,-1]

        lengths = []
        lengthpos = []
        gray_scale = []
        gray_scale_pos = []

        up_spikes = abs(np.convolve(strip_values, kernel,
                        "same")) > threshold

        #plt.clf()
        #plt.plot(up_spikes*30)
        #plt.savefig('gs_test3.png')

        for x in xrange(up_spikes.shape[0]):

            if up_spikes[x] == True:

                if (x - previous_spike) > low_length_tolerance:

                    lengths.append(x - previous_spike)
                    lengthpos.append(x)
                    previous_spike = x

                elif len(lengths) == 0:

                    previous_spike = x

        if len(lengths) > 0:

            tmpA = np.array(lengths)
            tmpLength = np.median(tmpA)
            found_first = False
            #print "* mLength", tmpLength, "Array:", lengths
            skip_next_pos = False

            for pos in xrange(len(lengths)):

                if not skip_next_pos:

                    #print pos, lenthspos[pos], lengths[pos]

                    if low_length_tolerance < lengths[pos] < \
                                    high_length_tolerance:

                        found_first = True

                        gray_scale.append(strip_values[lengthpos[pos] -
                            tmpLength * 3 / 4: lengthpos[pos] -
                            tmpLength * 1 / 4].mean())

                        gray_scale_pos.append(lengthpos[pos] - tmpLength
                            * 1 / 2)

                    elif found_first and low_length_tolerance < \
                            lengths[pos] / 2 < high_length_tolerance:

                        gray_scale_pos.append(lengthpos[pos] -
                                    tmpLength * 3 / 2)

                        gray_scale.append(strip_values[lengthpos[pos] -
                            tmpLength * 7 / 4: lengthpos[pos] -
                            tmpLength * 5 / 4].mean())

                        gray_scale_pos.append(lengthpos[pos] -
                            tmpLength * 1 / 2)

                        gray_scale.append(strip_values[lengthpos[pos] -
                            tmpLength * 3 / 4: lengthpos[pos] -
                            tmpLength * 1 / 4].mean())

                    elif found_first and low_length_tolerance < \
                            lengths[pos] / 3 < high_length_tolerance:

                        gray_scale.append(strip_values[lengthpos[pos] -
                            tmpLength * 11 / 4: lengthpos[pos] -
                            tmpLength * 9 / 4].mean())

                        gray_scale_pos.append(lengthpos[pos] -
                            tmpLength * 5 / 2)

                        gray_scale.append(strip_values[lengthpos[pos] -
                            tmpLength * 7 / 4: lengthpos[pos] -
                            tmpLength * 5 / 4].mean())

                        gray_scale_pos.append(lengthpos[pos] -
                            tmpLength * 3 / 2)

                        gray_scale.append(strip_values[lengthpos[pos] -
                            tmpLength * 3 / 4: lengthpos[pos] -
                            tmpLength * 1 / 4].mean())

                        gray_scale_pos.append(lengthpos[pos] -
                            tmpLength * 1 / 2)

                    elif found_first:

                        if pos + 1 < len(lengths) and \
                                low_length_tolerance < (lengthpos[pos + 1] -
                                lengthpos[pos - 1]) < high_length_tolerance:

                            gray_scale.append(strip_values[lengthpos[pos - 1] +
                                tmpLength * 1 / 4: lengthpos[pos - 1] +
                                tmpLength * 3 / 4].mean())

                            gray_scale_pos.append(lengthpos[pos - 1] +
                                tmpLength * 1 / 2)

                            skip_next_pos = True

                        else:

                            logging.warning("* Lost track after " + str(pos) +
                                "sections... extrapolating")

                            for pp in xrange(target_count - len(gray_scale)):

                                gray_scale.append(
                                    strip_values[lengthpos[pos - 1] +
                                    tmpLength * pp + tmpLength * 1 / 4:
                                    lengthpos[pos - 1] +
                                    tmpLength * pp +
                                    tmpLength * 3 / 4].mean())

                                gray_scale_pos.append(lengthpos[pos - 1] +
                                    tmpLength * pp + tmpLength * 1 / 2)

                            break

                else:

                    skip_next_pos = False

        #plt.plot(np.array(gray_scale_pos),np.array(gray_scale),'w*')
        #print " done spikes!"
        if len(gray_scale) > target_count:

            gray_scale_pos = gray_scale_pos[:target_count]
            gray_scale = gray_scale[:target_count]

        #If something bad happens towards the end it fills up with zeros
        #It offsets the X-values to match actual positions along the
        #excised strip

        for i, pos in enumerate(gray_scale):

            if np.isnan(pos):

                gray_scale[i] = 0.0

            gray_scale_pos[i] += rect[0][0]

        #plt.clf()
        #plt.plot(np.array(gray_scale_pos), np.array(gray_scale))
        #plt.savefig('gs_test4.png')

        #print gray_scale_pos
        #print gray_scale
        return gray_scale_pos, gray_scale


"""
FROM resource_image
CLASS: N/A
"""

def Quick_Invert_To_Tiff(source_path, target_path):

    import PIL.ImageOps

    try:

        im = Image.open(source_path)

    except:

        logging.error("Could not open source")

        return -1

    inv_im = PIL.ImageOps.invert(im)

    try:

        inv_im.save(target_path)

    except:

        logging.error("Could not save inverted image at " + str(target_path))

        return -1

    return True


def Quick_Rotate(source_path, target_path):

    try:

        im = Image.open(source_path)

    except:

        logging.error("Could not open source")

        return -1

    rot_im = im.rotate(90)

    try:

        rot_im.save(target_path)

    except:

        logging.error("Could not save inverted image at " + str(target_path))

        return -1

    return True



"""
FROM analysis_grid_cell_dissection
CLASS: Blob
"""

    def get_gaussian_probabilities(self):

        #P(X) = exp(-(X-m)^2/(2s^2))/(s sqrt(2*pi))
        pass

    def detect_fill(self, prob_array):
        """This function is not in use and should not be called"""

        return None

        self.filter_array = np.zeros(self.filter_array.shape)
        still_blob = True
        seed = prob_array.argmax()
        self.filter_array[seed] = True

        while still_blob:

            pass

        self.filter_array[seed] = False


"""
FROM analysis_grid_cell
CLASS: Grid_Cell
"""
    def detach_analysis(self, blob=True, background=True, cell=True):
        """detach_analysis disconnects the analysis modules specified
        from the Grid_Cell instance.

        Function has three optional boolean arguments:

        @blob           Detaches blob analysis (default)
                        This also detaches background

        @background     Detaches background analysis (default)

        @cell           Detaches cell analysis (default)"""

        if blob:

            self._analysis_items['blob'] = None
            self._analysis_items['background'] = None

        if background:

            self._analysis_items['background'] = None

        if cell:

            self._analysis_items['cell'] = None

"""
FROM analysis_grid_cell
CLASS: N/A
"""

def crop(A, rect):
    """ Crop numpy matrix/array A with rect = (x,y,width,height) where (x,y)
    is the topLeft corner of the rect """

    return A[rect[1]: (rect[3] + rect[1]), rect[0]: (rect[2] + rect[0])]


def compute_rect(center, rectSize):
    """ Typically you call this function with rectSize = interDist which is
    the default grid-rect size, but it might be the case that the size is
    set to smaller manually by invoking method setRectSize in class GridArray.

    rectSize can also be a tuple (width,height)

    for even, integer valued rectSize, the topLeft corner of the resulting
    rectangle will be center-rectSize/2 , which results in a rect
    "biased upwards to the left"

    if rectSize is empty or has negative elements, an ndarray
    with -1 elements is returned"""

    center = np.asarray(center)
    #print rectSize
    if np.isscalar(rectSize):

        rectSize = np.asarray([rectSize, rectSize])

    else:

        rectSize = np.asarray(rectSize)

    if (rectSize < 0).any() or rectSize.size == 0:

        return np.asarray([-1, -1, -1, -1])

    topLeft = center - rectSize / 2.0

    return np.asarray(tuple(topLeft) + tuple(rectSize))

#
# SPREADSHEET COMPATIBILITY FUNCTIONS
#


def ystring(ycoor):
    """'ystring' produces a string representation for the ycoor (integer >=1)
    that corresponds to Excels column annotation.
    1 (first column) => 'A'
    2                => 'B'
    ...
    26               => 'Z'
    27               => 'AA'
    28               => 'AB' (etc

    Note that this is NOT the typical representation in another base/alphabet,
    since in that case 'A' should correspond to 0, 'B' to 1, and 'Z' to 25, and
    'BA' to 26 since these would be short-hand to 'AAA' <=> 0, 'AAB' <=> 1,
    'AAZ'<=> 25 and 'ABA' to 26.
    Of course this could be offset:ed with +1, but it does not change
    the fact that it is another algorithm for columns/rows >=27"""

    if ycoor < 27:

        return chr(65 + ycoor - 1)

    else:

        y = ycoor - 1
        next = y / 26
        rest = y % 26

        return ystring(next) + chr(65 + rest)


def create_id_tag(xcoor, ycoor, nrCols, nrRows):

    #x = xcoor-1;
    nrDigits = math.floor(math.log10(nrCols) + 1)
    template = '%0' + ('%d' % (nrDigits)) + 'd'

    return ystring(ycoor) + (template % (xcoor))



"""
FROM: analysis_grid_array_dissection
CLASS: Grid_Analysis
"""

    def get_spikes(self, dimension, im=None, visual=False, verbose=False,
             use_otsu=True, median_coeff=0.99, manual_threshold=None):
        """
        get_spikes returns a spike list for a dimension of an image array

        The function takes the following arguments:

        @dimension  The dimension to be analysed (0 or 1)

        @im         An image numpy array, if left out previously loaded
                    image will be used

        @visual     Plot the results (only possible when running
                    the script from prompt)

        @verbose   Do a whole lot of print out of everything to
                    debug what goes wrong.

        @use_otsu   Using the Otsu algorithm to set the threshold used in
                    spike detection (default True). If Otsu is not used,
                    the median coefficient is used.

        @median_coeff   A float that is multiplied to the median of the
                        1D flattned image to get a threshold if otsu is not
                        used.
        """

        if im == None:

            im = self.im

        im_1D = im.mean(axis=dimension)

        if use_otsu:

            self.histogram.re_hist(im_1D)
            self.threshold = hist.otsu(histogram = self.histogram)

        elif manual_threshold:

            self.threshold = manual_threshold

        else:

            if median_coeff is None:

                median_coeff = 0.99

            self.threshold = np.median(im_1D) * median_coeff

        im_1D2 = (im_1D < self.threshold).astype(int)
        if visual:
            Y = im_1D2 * 100
            plt.plot(np.arange(len(im_1D)), im_1D, 'b-')
            plt.plot(np.arange(len(im_1D2)), Y, 'g-')
            #print self.threshold, median_coeff
            plt.axhline(y=self.threshold, color='r')
            plt.axhline(y=np.median(im_1D), color='g')

        #kernel = [-1,1]
        #spikes = np.convolve(im_1D, kernel, 'same')

        spikes_toggle_up = []
        spikes_toggle_down = []

        spikes_toggle = False

        for i in xrange(len(im_1D2)):

            if im_1D2[i] and not spikes_toggle:

                spikes_toggle = True
                spikes_toggle_up.append(i)

            elif not im_1D2[i]:

                if spikes_toggle == True:
                    spikes_toggle_down.append(i)

                spikes_toggle = False

        if len(spikes_toggle_down) != len(spikes_toggle_up):

            spikes_toggle_up = spikes_toggle_up[: len(spikes_toggle_down)]

        self.logger.debug("GRID CELL get_spikes, %d long %d downs %d ups." % \
            (len(im_1D2), len(spikes_toggle_down), len(spikes_toggle_up)))

        stt = (np.array(spikes_toggle_up) + np.array(spikes_toggle_down)) / 2

        if visual:

            Y = np.ones(len(stt)) * 80

            plt.plot(stt, Y, 'b.')
            plt.show()

        spike_f = stt[1:] - stt[: -1]

        return stt[1:], spike_f


"""
FROM: analysis_grid_array_dissection
CLASS: Grid_Analysis
"""

    def get_analysis(self, im, history=[]):
        """
        get_analysis is a convenience function for get_spikes and
        get_signal_position_and_frequency functions run on both
        dimensions of the image.

        (This function sets the self.im for get_spikes.)

        The function takes the following arguments:

        @im         An array / the image

        @pinning_matrix  A list/tuple/array where first position is
                        the number of rows to be detected and second
                        is the number of columns to be detected.

        @use_otsu   Causes thresholding to be done by Otsu
                    algorithm (Default)

        @median_coeff       Coefficient to threshold from the
                            median when not using Otsu.

        @verbose   If a lot of things should be printed out

        @visual     If visual information should be presented.

        @history    A history of the top-left positions selected
                    for the particular format for the particular plate

        The function returns two arrays, one per dimension, of the
        positions of the spikes and a quality index
        """


        self.im = im
        positions = [None, None]
        measures = [None, None]
        best_fit_frequency = [None, None]
        best_fit_positions = [None, None]
        adjusted_by_history = False

        R = 0

        if history is not None and len(history) > 0:

            history_rc = (np.array([h[1][0] for h in history]).mean(),
                np.array([h[1][1] for h in history]).mean())

            history_f = (np.array([h[2][0] for h in history]).mean(),
                np.array([h[2][1] for h in history]).mean())

        else:

            history_rc = None
            history_f = None

        """
        fig = plt.figure(10)
        fig.add_subplot(2,2,1).imshow(im, cmap=plt.cm.Greys)
        fig.add_subplot(2,2,2).plot(im.mean(0))
        t = hist.otsu(hist.Histogram(im.mean(0)))
        fig.gca().plot((0, im.shape[1]),(t, t))
        fig.gca().set_title("Dim 0 ({0})".format(pinning_matrix[dim_order[0]]))
        fig.add_subplot(2,2,3).plot(im.mean(1))
        t = hist.otsu(hist.Histogram(im.mean(1)))
        fig.gca().plot((0, im.shape[0]),(t, t))
        fig.gca().set_title("Dim 1 ({0})".format(pinning_matrix[dim_order[1]]))
        fig.show() 
        raw_input("Im {0}, Pin {1}, Order {2}".format(im.shape,
                pinning_matrix, dim_order))
        """

        #Obtaining current values by pinning grid dimension order

        np.save("_grid_im.npy", im)

        for dim in xrange(2):

            positions[dim], measures[dim] = \
                    self._get_new_spikes(
                    self.dim_order[dim]-1,
                    im=im)

            """
            positions[dim], measures[dim] = \
                    self.get_spikes(
                    self.dim_order[dim]-1,
                    im=im, visual=self.visual, verbose=self.verbose,
                    use_otsu=self.use_otsu, median_coeff=self.median_coeff,
                    manual_threshold=self.manual_threshold)
            """

            self.logger.info(
                "GRID ARRAY, Peak positions %sth dimension:\n%s" %\
                (str(dim), str(positions[dim])))

            best_fit_frequency[dim] = r_signal.get_signal_frequency(
                positions[dim])

            if best_fit_frequency[dim] is not None \
                                and history_f is not None:

                if abs(best_fit_frequency[dim] /\
                            float(history_f[dim]) - 1) > 0.1:

                    self.logger.warning(
                            ('GRID ARRAY, frequency abnormality for ' +\
                            'dimension {0} (Current {1}, Expected {2}'.format(
                            dim, best_fit_frequency[dim],
                            history_f)))

                    adjusted_by_history = True
                    best_fit_frequency[dim] = history_f[dim]

            best_fit_positions[dim] = r_signal.get_true_signal(
                im.shape[self.dim_order[dim]], self.pinning_matrix[dim],
                positions[dim],
                frequency=best_fit_frequency[dim],
                offset_buffer_fraction=0.5)

            if best_fit_positions[dim] is not None and \
                                        history_rc is not None:

                goodness_of_signal = r_signal.get_position_of_spike(
                    best_fit_positions[dim][0], history_rc[dim],
                    history_f[dim])

                if abs(goodness_of_signal) > 0.2:

                    self.logger.warning(("GRID ARRAY, dubious pinning " + \
                        "position for  dimension {0} (Current signal " + \
                        "start {1}, Expected {2} (error: {3}).".format(
                        dim, best_fit_positions[dim][0],
                        history_rc[dim], goodness_of_signal)))

                    adjusted_by_history = True

                    new_fit = r_signal.move_signal(
                        list(best_fit_positions[dim]),
                        [-1 * round(goodness_of_signal)], freq_offset=0)

                    if new_fit is not None:

                        self.logger.warning(
                            "GRID ARRAY, remapped signal for " +\
                            "dimension {0} , new signal:\n{1}".format(
                            dim, list(new_fit)))

                        best_fit_positions[dim] = new_fit[0]

            self.logger.info("GRID ARRAY, Best fit:\n" +\
                "* Elements: " + str(self.pinning_matrix[dim]) +\
                "\n* Positions:\n" + str(best_fit_positions[dim]))

            #DEBUGHACK
            #visual = True
            #DEBUGHACK - END

            if self.visual and best_fit_positions[dim] is not None:

                Y = np.ones(len(best_fit_positions[dim])) * 50
                Y2 = np.ones(positions[dim].shape) * 100
                plt.clf()

                if self.dim_order[dim] == 1:

                    plt.imshow(im[:, 900: 1200].T, cmap=plt.cm.gray)

                else:

                    plt.imshow(im[300: 600, :], cmap=plt.cm.gray)

                plt.plot(positions[dim], Y2, 'r*',
                    label='Detected spikes', lw=3, markersize=10)

                plt.plot(np.array(best_fit_positions[dim]),\
                    Y, 'g*', label='Selected positions', lw=3, markersize=10)

                plt.legend(loc=0)
                plt.ylim(ymin=0, ymax=150)
                plt.show()

            if best_fit_positions[dim] != None:

                #Comparing to previous
                if self.best_fit_positions != None:

                    if self.best_fit_positions[dim] != None:

                        R += ((best_fit_positions[dim] - \
                            self.best_fit_positions[dim]) ** 2).sum() / \
                            float(pinning_matrix[dim])

                        #Updating previous
                        self.logger.info(
                            "GRID ARRAY, Got a grid R at, {0}".format(R))

        if R < 20 and best_fit_positions[0] != None and \
                             best_fit_positions[1] != None:

            #self.best_fit_start_pos = best_fit_start_pos
            self.best_fit_frequency = best_fit_frequency
            self.best_fit_positions = best_fit_positions
            self.R = R

        else:

            self.R = -1

        if self.best_fit_positions == None:

            return None, None, None, adjusted_by_history

        else:

            ret_tuple = (self.best_fit_positions[0],
                    self.best_fit_positions[1], self.R,
                    adjusted_by_history)

            return ret_tuple
"""
FROM: analysis_grid_array.py
CLASS: Grid_Array
"""

    def get_analysis_old(self, im, gs_fit=None, gs_values=None, use_fallback=False,
                use_otsu=True, median_coeff=None, verbose=False, visual=False,
                watch_colony=None, suppress_analysis=False,
                save_grid_name=None, identifier_time=None,
                save_anime_name=None, timestamp=None, animate=False):

        """Returns analysis!
        @param im: An array / the image

        @param gs_fit : An array of the fitted coefficients for the grayscale

        @param gs_values : An array of the grayscale pixelvalues, if
        submittet gs_fit is disregarded

        @param use_otsu : Causes thresholding to be done by Otsu
        algorithm (Default)

        @param median_coeff : Coefficient to threshold from the
        median when not using Otsu.

        @param verbose : If a lot of things should be printed out

        @param visual : If visual information should be presented.

        @param save_grid_name : A custom name for the saved image, if none
        is submitted, it will be grid.png in current directory.

        @param identifier_time : A time index to update the identifier with

        @param save_anime_name : Path to where to save animation figure

        The function returns two arrays, one per dimension, of the
        positions of the spikes and a quality index
        """

        if identifier_time is not None:
            self._identifier[0] = identifier_time

        debug_per_plate = False

        #DEBUGHACK
        #visual = True
        #verbose = True
        #debug_per_plate = True
        #DEBUGHACK - END

        self.watch_source = None
        self.watch_scaled = None
        self.watch_blob = None
        self.watch_results = None

        if debug_per_plate:

            raw_input("Waiting to start next plate (press Enter)")

        if self._best_fit_columns is None:

            if not self.set_grid(im, save_grid_name=save_grid_name,
                    use_otsu=use_otsu, median_coeff=median_coeff,
                    verbose=verbose, visual=visual):

                self.logger.critical('Failed to set grid on ' + \
                        '{0} and none to use'.format(self._identifier))

                return None

        if (min(self._pinning_matrix) == self._pinning_matrix[1]) != \
            (min(im.shape) == im.shape[1]):

            ax1 = 0
            ax2 = 1
            s_bfc = self._best_fit_rows
            s_bfr = self._best_fit_columns
            s_gcs = [self._grid_cell_size[1], self._grid_cell_size[0]]

        else:

            ax1 = 1
            ax2 = 0
            s_bfr = self._best_fit_rows
            s_bfc = self._best_fit_columns
            s_gcs = self._grid_cell_size

        #total_steps = float(self._pinning_matrix[0] * self._pinning_matrix[1])

        #DEBUG PLOT GRID
        #debug_fig = plt.figure()
        #debug_ax = debug_fig.add_subplot(1,1,1)
        #debug_ax.imshow(im)
        #if self._best_fit_rows is not None:
            #debug_cols = (0, im.shape[0])
            #for debug_row in self._best_fit_rows:
                #debug_ax.plot((debug_row, debug_row),debug_cols,'k')
        #if self._best_fit_columns is not None:
            #debug_rows = (0, im.shape[1])
            #for debug_col in self._best_fit_columns:
                #debug_ax.plot(debug_rows,(debug_col,debug_col),'k')
        #debug_fig.show()
        #self.logger.warning("Pining (R,C) " + \
            #"{0} best_rows {1} best_cols {2}".format(
            #self._pinning_matrix, self._best_fit_rows, best_fit_columns))
        #raw_input("> ")
        #DEBUG PLOT GRID END

        #Normalising towards grayscale before anything is done on the colonies
        transformation_matrix = None
        

        #KODAK neutral scale
        if self._parent is not None:

            gs_indices = self._parent.gs_indices

        else:

            gs_indices = None

        if gs_values == None:

            transformation_matrix = self.get_transformation_matrix(\
                gs_fit=gs_fit, gs_indices=gs_indices)

        else:

            transformation_matrix = self.get_transformation_matrix(\
                gs_values=gs_values, gs_indices=gs_indices)

        #print "\n***Transformation matrix"
        #print transformation_matrix

        #if watch_colony != None:
        #    ul = self._grid_cells[watch_colony[1]][watch_colony[2]]\
                #.get_top_left()
        #    lr = self._grid_cells[watch_colony[1]][watch_colony[2]]\
                #.get_bottom_right()
        #    self.watch_source = im[ul[1]:lr[1],ul[0]:lr[0]]

        #if transformation_matrix != None:
            #There's probably some faster way
            #for x in xrange(im.shape[0]):
                #for y in xrange(im.shape[1]):
                    #im[x,y] = transformation_matrix[im[x,y]]
        #print "*** Analysing grid:"

        tf_im = np.zeros(s_gcs, dtype=np.float64)

        for row in xrange(self._pinning_matrix[ax1]):

            for column in xrange(self._pinning_matrix[ax2]):

                if suppress_analysis == False or (watch_colony != None and \
                        watch_colony[1] == row and watch_colony[2] == column):

                    _cur_gc = self._grid_cells[row][column]

                    row_min = s_bfr[row]
                    col_min = s_bfc[column]

                    _cur_gc.set_center(
                                    (row_min,
                                    col_min),
                                    s_gcs)

                    if transformation_matrix is not None:

                        #There's probably some faster way
                        self.logger.debug(
                                "ANALYSIS GRID ARRAY Transforming -> Kodak")

                        for x in xrange(tf_im.shape[ax1]):

                            x2 = int(round(row_min)) + x
                                        #- s_gcs[0] / 2.0)) + x

                            for y in xrange(tf_im.shape[ax2]):

                                y2 = int(round(col_min)) + y
                                        #- s_gcs[1] / 2.0)) + y

                                try:
                                    tf_im[x, y] = \
                                        transformation_matrix[im[x2, y2]]

                                except IndexError:

                                    self.logger.critical(\
                                        "Index Error:An image has been " + \
                                        " saved as gridding_error.png\n" + \
                                        "tf_im.shape {0} vs ({1}, {2})".format(
                                        tf_im.shape, x, y) + \
                                        "and im.shape {0} vs ".format(im.shape
                                        ) + "({0}, {1})\nbest_fit ".format(
                                        x2, y2) + \
                                        "({0}, {1}) size ({2}, {3}) ".format(
                                        col_min,
                                        row_min,
                                        s_gcs[ax1],
                                        s_gcs[ax2]) + \
                                        "from {0}:{1}:{2}".format(
                                        self._identifier, (row, 
                                        self._pinning_matrix[ax1]), (column,
                                        self._pinning_matrix[ax2])))

                                    grid_image = plt.figure()
                                    grid_plot = grid_image.add_subplot(111)
                                    grid_plot.imshow(im, cmap=plt.cm.Greys)
                                    grid_plot.set_xlim(0, im.shape[ax1-1])
                                    grid_plot.set_ylim(0, im.shape[ax2-1])

                                    for row in xrange(self._pinning_matrix[ax1]):

                                        grid_plot.plot(
                                            np.ones(
                                            len(s_bfc)) * \
                                            s_bfr[row],
                                            np.array(s_bfc),
                                            'r-')

                                        for column in xrange(
                                                self._pinning_matrix[ax2]):

                                            grid_plot.plot(
                                                np.array(s_bfr),
                                                np.ones(
                                                len(s_bfr)) * \
                                                s_bfc[column],
                                                'r-')

                                    grid_plot.add_patch(plt.Rectangle((y2,x2),
                                        tf_im.shape[ax1-1],
                                        tf_im.shape[ax2-1],
                                        ls='solid', lw=2,
                                        fill=False, ec=(0.9, 0.9, .1, 1)))

                                    grid_image.savefig("gridding_error.png")

                                    err_str = "Image showing the grid that " +\
                                                "caused it: gridding_error.png"

                                    raise Exception(IndexError, err_str)

                                    sys.exit()

                    else:

                        self.logger.critical("ANALYSIS GRID ARRAY Lacks" + \
                                " transformation possibilities")

                    _cur_gc.set_data_source(tf_im)

                    #if watch_colony != None:
                        #if row == watch_colony[1] and column \
                                    #== watch_colony[2]:

                            #self.watch_scaled = tf_im
                            #if self.watch_scaled.sum() == \
                                    #(self.watch_scaled > 0).sum():

                                ###DEBUG WHAT IS THE GRID ARRAY
                                #plt.clf()
                                #plt.imshow(self.watch_scaled, title='Grid')
                                #plt.show()
                                ###END DEBUG CODE

                    #This happens only the first time
                    if self._first_analysis:

                        _cur_gc.attach_analysis(
                                blob=True, background=True, cell=True,
                                use_fallback_detection=use_fallback,
                                run_detect=False)


                    #Gather info on the blob
                    self._features[row][col] = _cur_gc.get_analysis()

                    if watch_colony is not None \
                                and row == watch_colony[1] \
                                and column == watch_colony[2]:

                        blob = _cur_gc.get_item('blob')

                        background = _cur_gc.get_item('background')

                        if animate:
                            #plt.clf()
                            fig = plt.figure()
                            #gs = gridspec.GridSpec(2, 2)
                            #ax = fig.add_subplot(221, title="Blob")
                            #fig.gca().imshow(blob.filter_array)
                            #ax = fig.add_subplot(222, title ="Background")
                            #fig.gca().imshow(_cur_gc.get_item('background')\
                                #.filter_array)

                            #DEBUG CODE START
                            #blob = _cur_gc.get_item('blob')
                            #plt.clf()
                            #plt.subplot(211, title='filter all done')
                            #plt.imshow(blob.filter_array)
                            #plt.subplot(212, title='image')
                            #plt.imshow(blob.grid_array, vmax=3500, vmin=0)
                            #plt.show()
                            #DEBUG CODE END

                            ax = fig.add_subplot(221, title="Image t={0}".\
                                    format(self._identifier[0]))

                            ax_im = fig.gca().imshow(blob.grid_array, vmin=0,
                                    vmax=3500)

                            ax.get_xaxis().set_visible(False)
                            ax.get_yaxis().set_visible(False)
                            #fig.colorbar(ax_im,ax)

                            ax = fig.add_subplot(223, title="Blob")
                            ax_im = fig.gca().imshow(blob.filter_array)
                            ax.get_xaxis().set_visible(False)
                            ax.get_yaxis().set_visible(False)

                            ax = fig.add_subplot(224, title="Background")
                            ax_im = fig.gca().imshow(background.filter_array)
                            ax.get_xaxis().set_visible(False)
                            ax.get_yaxis().set_visible(False)

                        if self._old_blob_img is not None and \
                                    self._old_timestamp is not None:

                            blob_diff = blob.get_diff(self._old_blob_img,
                                self._old_blob_filter)

                            #onion2 = blob.get_onion_values(blob_diff,
                                #self._old_blob_filter,
                                #2)
                            #onion2t = blob.get_onion_values(blob_diff,
                                #blob.filter_array,
                                #2)
                            #onion4 = blob.get_onion_values(blob_diff,
                                #self._old_blob_filter,
                                #4)

                            onion6 = blob.get_onion_values(blob_diff,
                                            self._old_blob_filter,
                                            6)

                            self._onion_store.insert(0,
                                    ((onion6[-1, 0] / onion6[-1, 1]) / \
                                    ((self._old_timestamp - timestamp) / \
                                    (3600.0))))

                            self._onion_times.insert(0, (timestamp + \
                                    (self._old_timestamp - timestamp) / 2.0) /\
                                    (3600.0))

                                #onion2t[-1,0]/float(onion2t[-1,1])))
                                #onion4[-1,0]/float(onion4[-1,1]),
                                #onion6[-1,0]/float(onion6[-1,1])))

                            #fig3 = plt.figure()
                            #fig3.add_subplot(2,2,1, title = 'now')
                            #fig3.gca().imshow(blob.grid_array, vmin=0,
                                    #vmax=3500)
                            #fig3.add_subplot(2,2,2, title = 'previous')
                            #fig3.gca().imshow(self._old_blob_img, vmin=0,
                                    #vmax=3500)
                            #fig3.add_subplot(2,2,3, title = 'previous')
                            #fig3.gca().imshow(im, vmin=0, vmax=3500)
                                #uncertain of if everything is cool here
                            #fig3.gca().plot([y2 - self._grid_cell_size[0]/2],
                                #[x2[0] - self._grid_cell_size[1]/2], 'ro')
                            #fig3.show()
                            #raw_input("now max {0} vs old {1}> ".format(
                                    #blob.grid_array.max(),
                                    #self._old_blob_img.max()))

                            if self._identifier[0] == 0:

                                onion_times = np.asarray(self._onion_times)
                                onion_store = np.asarray(self._onion_store)
                                onion_labels = ['T2 outer using true dt',
                                    'T2 outer using equal dt']
                                    #'Thickness 4, outer',
                                    #'Thickness 6, outer']

                                np.save('onion_start_val', np.array(
                                        (np.log2(onion6[-1, 0]),)))
                                np.save('onion_store_arr', onion_store)
                                np.save('onion_times_arr', onion_times)

                                #fig2 = plt.figure()
                                #fig2.gca().set_title("1st Derivative of "
                                   #"Outer Onion Peels (t vs t+1 onionrings")

                                #for i in xrange(onion_store.shape[0]):

                                    #fig2.gca().plot(
                                        #np.arange(onion_store.size),
                                        #np.arange(onion_store.shape[0]),
                                        #onion_store, '-',
                                        #label=onion_labels[0])

                                #fig2.gca().set_xlabel("Time indices")
                                #fig2.gca().set_ylabel("Avg cell estimate" +
                                    #" diff to next time-pt")
                                #fig2.gca().legend(loc=0)
                                #fig2.savefig("onion.png")

                            if animate:

                                ax = fig.add_subplot(222,
                                        title="Delta Cells Image")

                                ax_im = fig.gca().imshow(blob_diff,
                                        vmin=-700, vmax=700,
                                        cmap=plt.cm.RdYlGn)

                                ax.get_xaxis().set_visible(False)
                                ax.get_yaxis().set_visible(False)
                                fig.colorbar(ax_im)  # fraction=2)

                                #ax = fig.add_subplot(224,
                                        #title="Onion Avg Residuals")
                                #ax_im = fig.gca().plot(
                                        #np.arange(onion6.shape[0]),
                                        #onion6[:,0]/onion6[:,1]\
                                        #.astype(np.float64),
                                        #'g-')
                                #ax.set_xlabel(
                                    #'Onion layer index (0 = center of blob)')
                                #ax.set_ylabel('Avg residual(t+1 - t')
                                #ax.set_autoscalex_on(False)
                                #ax.set_autoscaley_on(False)
                                #ax.set_ylim((-150,300))
                                #ax.set_xlim((0,5))

                        else:

                            self._onion_times = []
                            self._onion_store = []

                        self._old_blob_img = blob.grid_array.copy()
                        self._old_blob_filter = blob.filter_array.copy()

                        if animate:
                            #ax = fig.add_subplot(313, title = "Growth-curve")
                            #fig.gca().semilogy(self.track_times,
                                #self.track_values,
                                #'b-', basey=2)
                            #self.track_times.append(self._identifier[0])
                            #self.track_values.append(
                                #self._features[row][column]\
                                #['blob']['pixelsum'])
                            #fig.gca().semilogy((self.track_times[-1],), (
                                #self.track_values[-1],),'ro', basey=2)
                            #ax.set_yticklabels(("0","2^5","1^6"))
                            #ax.set_yscale('log', basey=2)
                            #ax.set_yticks((0,5,6))
                            #plt.xlim(0, self.track_times[0])
                            #plt.ylim(0, max(self.track_values))
                            fig.savefig(save_anime_name)
                            del fig

                    if watch_colony != None:
                        if row == watch_colony[1] and \
                                    column == watch_colony[2]:

                            self.watch_blob = \
                                _cur_gc.get_item('blob').filter_array.copy()

                            self.watch_scaled = \
                                _cur_gc.get_item('blob').grid_array.copy()

                            self.watch_results = self._features[row][column]

        self._old_timestamp = timestamp

        #print str(((row+1)*self._pinning_matrix[1]+column+1)/total_steps)+"%"
        self._first_analysis = False

        return self._features
"""
FROM: analysis.py
CLASS: Grid_Image
DEF: get_analsys
"""

        if save_graph_image:

            for ga_i, grid_array in enumerate(self._grid_arrays):

                im = self.get_im_section(features[ga_i], scale_factor)

                cur_graph_name = save_graph_name + str(ga_i) + ".png"

                if grid_lock == False or grid_array._best_fit_rows is None:

                    grid_array.set_grid(im,
                        save_grid_name=(save_graph_image is None and
                        cur_graph_name or None),
                        use_otsu=use_otsu, median_coeff=None,
                        verbose=False, visual=False, dont_save_grid=False)

"""
FROM: analysis_grid_cell.py
CLASS: Grid_Cell
"""


    def set_rect_size(self, rect_size=None):

        if rect_size == None:
            rect_size = self.data_source.shape

        if self.center.all(-1):
            self.center = np.array(map(lambda x: x / 2.0, rect_size))

        self.rect = compute_rect(self.center, rect_size)

    def set_center(self, center, rect_size=None):

        # Set the new center:
        self.center = np.asarray(center)

        # find the rectSize:
        if rect_size is None:

            rect_size = self.get_rect_size()

        # Set rectSize, which also corrects for the new center:
        self.set_rect_size(rect_size)

    def set_offset(self, offset_vec):

        vec = np.asarray(offset_vec)
        vec.shape = self.center.shape
        self.center += vec
        self.set_center(self.center)

    def get_first_dim_as_tuple(self):

        return (self.rect[0], self.rect[0] + self.get_width())

    def get_second_dim_as_tuple(self):

        return (self.rect[1], self.rect[1] + self.get_height())

    def get_rect_size(self):

        return self.rect[2: 4]

    def get_rect(self):

        return self.rect

    def get_top_left(self):

        return self.rect[0: 2]

    def get_bottom_right(self):

        widthHeight = np.maximum(self.rect[2: 4] - 1, 0)

        return self.rect[0: 2] + widthHeight

    def get_bottom_left(self):

        x = self.rect[0]
        height = max(self.rect[3] - 1, 0)
        y = self.rect[1] + height

        return np.array([x, y])

    def get_top_right(self):

        width = np.max(self.rect[2] - 1)
        x = self.rect[0] + width
        y = self.rect[1]

        return np.array([x, y])

    def get_width(self):

        return self.rect[2]

    def get_height(self):

        return self.rect[3]

    def get_center(self):

        return self.center


"""
FROM: analysis_grid_cell_disection.py 
CLASS: Background 
DEF: detect
"""

            #kernel = get_round_kernel(radius=9)

            #self.filter_array = binary_erosion(self.filter_array,
            #                    structure=kernel, border_value=1)

            ###DEBUG CODE
            #print "Bg area", np.sum(self.filter_array),
            #print "of which shared with blob",
            #print np.sum(self.filter_array * self.blob.filter_array)
            #print "I am", self._identifier
            #if True:
            #if self._identifier[0][0] == 0 or self._identifier[0][0] % \
                #round(self._identifier[0][0]**0.5) == 0 or \
                #abs(self._identifier[0][0] - 189) < 3:

                #from matplotlib import pyplot as plt
                #plt.clf()
                #fig = plt.figure()
                #ax = fig.add_subplot(221, title="Blob")
                #fig.gca().imshow(self.blob.filter_array)
                #ax = fig.add_subplot(222, title ="Background")
                #fig.gca().imshow(self.filter_array)
                #ax = fig.add_subplot(223, title = "Image")
                #ax_im = fig.gca().imshow(self.grid_array, vmin=0, vmax=100)
                #fig.colorbar(ax_im)
                #fig.savefig("debug_cell_t" +\
                    # ("%03d" % self._identifier[0][0]))
            ###END DEBUG CODE

            #if remember_filter:

            #    self.old_filter = self.filter_array.copy()

"""
FROM: resource_calibration.py
CLASS: N/A
"""

#if filter_2 != "":
    #data_2 = np.asarray(data_list_2)
#
    ##GRAPH 1
    #plt.plot(data_1[data_1_joint_positions,1],data_2[data_2_joint_positions,1],'b.')
#
    #z1 = np.polyfit(data_1[data_1_joint_positions,1],data_2[data_2_joint_positions,1],1)
    #p1 = np.poly1d(z1)
    #xp = np.linspace(data_1[data_1_joint_positions,1].min(), data_1[data_1_joint_positions,1].max(), 100)
#
    #plt.plot(xp, p1(xp), 'g-', label='1nd deg')
    #plt.text(500, 1000, str(p1) + ", r: " + str(p1.r[0]))
#
    #plt.xlabel(filter_1)
    #plt.ylabel(filter_2)
    #plt.title("Comparison of independent measures")
    #plt.legend(loc=1)
    #plt.show()


#GRAPH 2
#z1 = np.polyfit(data_1[:,0], data_1[:,1],1)
#p1 = np.poly1d(z1)

#z2 = np.polyfit(data_1[:,0], data_1[:,1],2)
#p2 = np.poly1d(z2)

#z3 = np.polyfit(data_1[:,0], data_1[:,1],3)
#p3 = np.poly1d(z3)

#xp = np.linspace(data_1[:,0].min(), data_1[:,0].max(), 100)
#x_span = data_1[:,0].max() - data_1[:,0].max()
#y_span = data_1[:,1].max() - data_1[:,1].max()

plt.clf()
plt.plot(data_1[:,0], data_1[:,1], 'b.')
plt.plot(xp, p1(xp),'m-', label='1nd deg')
plt.plot(xp, p2(xp),'r-', label='2nd deg')
plt.plot(xp, p3(xp),'g-', label='3rd deg')
plt.xlabel("Mean Pixel-Darkening from Colony Growth in 'Kodak Space' (Larger negative number, more stuff on agar)")
plt.ylabel("Independet Cell Estimate per pixel")
plt.title(filter_1 + " based conversion to 'Cell Estimate Space'")
plt.legend(loc=0)
#plt.xlim(xmin=data_1[:,0].min() - x_span * 0.15, xmax=data_1[:,0].max() + x_span * 0.15)
#plt.ylim(ymin=data_1[:,1].min() - y_span * 0.15, ymax=data_1[:,1].max() + y_span * 0.15)
#plt.ylim(ymax=2100, ymin=-50)
#plt.xlim(xmax=5, xmin=-100)
plt.ylim(ymax=500, ymin=-50)
plt.xlim(xmax=5, xmin=-30)
plt.show()


if filter_2 != "":
    #GRAPH 3
    z1 = np.polyfit(data_2[:,0], data_2[:,1],1)
    p1 = np.poly1d(z1)

    z2 = np.polyfit(data_2[:,0], data_2[:,1],2)
    p2 = np.poly1d(z2)

    z3 = np.polyfit(data_2[:,0], data_2[:,1],3)
    p3 = np.poly1d(z3)

    xp = np.linspace(data_2[:,0].min(), data_2[:,0].max(), 100)
    x_span = data_2[:,0].max() - data_2[:,0].max()
    y_span = data_2[:,1].max() - data_2[:,1].max()

    plt.clf()
    plt.plot(data_2[:,0], data_2[:,1], 'b.')
    plt.plot(xp, p1(xp),'m-', label='1nd deg')
    plt.plot(xp, p2(xp),'r-', label='2nd deg')
    plt.plot(xp, p3(xp),'g-', label='3rd deg')
    plt.xlabel("Mean Pixel-Darkening from Colony Growth in 'Kodak Space' (Larger negative number, more stuff on agar)")
    plt.ylabel("Independet Cell Estimate per pixel")
    plt.title(filter_2 + " based conversion to 'Cell Estimate Space'")
    plt.legend(loc=0)
    #plt.xlim(xmin=data_1[:,0].min() - x_span * 0.15, xmax=data_1[:,0].max() + x_span * 0.15)
    #plt.ylim(ymin=data_1[:,1].min() - y_span * 0.15, ymax=data_1[:,1].max() + y_span * 0.15)
    plt.ylim(ymax=2100, ymin=-50)
    plt.xlim(xmax=5, xmin=-100)
    #plt.ylim(ymax=500, ymin=-50)
    #plt.xlim(xmax=5, xmin=-30)
    plt.show()
#GRAPH 2
#print data_1[:,1]
#data_1[:,1] = np.log(np.log(data_1[:,1]))
#print data_1[:,1]

#z1 = curve_fit(np.log,data_1[:,0], data_1[:,1])
#p1 = np.poly1d(z1)

#z2 = np.polyfit(data_1[:,0], data_1[:,1],2)
#p2 = np.poly1d(z2)

#z3 = np.polyfit(data_1[:,0], data_1[:,1],3)
#p3 = np.poly1d(z3)

#xp = np.linspace(data_1[:,0].min(), data_1[:,0].max(), 100)
#x_span = data_1[:,0].max() - data_1[:,0].max()
#y_span = data_1[:,1].max() - data_1[:,1].max()

#plt.clf()
#plt.plot(data_1[:,0], data_1[:,1], 'b.')
#plt.plot(xp, p1(xp),'m-', label='1nd deg')
#plt.plot(xp, p2(xp),'r-', label='2nd deg')
#plt.plot(xp, p3(xp),'g-', label='3rd deg')
#plt.xlabel("Mean Pixel-Darkening from Colony Growth in 'Kodak Space' (Larger negative number, more stuff on agar)")
#plt.ylabel("Log Independet Cell Estimate per pixel")
#plt.title(filter_1 + " based conversion to a logged 'Cell Estimate Space'")
#plt.legend(loc=0)
#plt.xlim(xmin=data_1[:,0].min() - x_span * 0.15, xmax=data_1[:,0].max() + x_span * 0.15)
#plt.ylim(ymin=data_1[:,1].min() - y_span * 0.15, ymax=data_1[:,1].max() + y_span * 0.15)
#plt.ylim(ymax=2100, ymin=-50)
#plt.xlim(xmax=5, xmin=-100)
#plt.show()

"""
FROM: analysis_grid_array_dissection.py
CLASS: Grid_Analysis
"""


    def get_analysis(self, im, pinning_matrix, use_otsu=True,
        median_coeff=None, verbose=False,
        visual=False, history=[], manual_threshold=None):
        """
        get_analysis is a convenience function for get_spikes and
        get_signal_position_and_frequency functions run on both
        dimensions of the image.

        (This function sets the self.im for get_spikes.)

        The function takes the following arguments:

        @im         An array / the image

        @pinning_matrix  A list/tuple/array where first position is
                        the number of rows to be detected and second
                        is the number of columns to be detected.

        @use_otsu   Causes thresholding to be done by Otsu
                    algorithm (Default)

        @median_coeff       Coefficient to threshold from the
                            median when not using Otsu.

        @verbose   If a lot of things should be printed out

        @visual     If visual information should be presented.

        @history    A history of the top-left positions selected
                    for the particular format for the particular plate

        The function returns two arrays, one per dimension, of the
        positions of the spikes and a quality index
        """

        self.im = im
        positions = [None, None]
        measures = [None, None]
        #best_fit_start_pos = [None, None]
        best_fit_frequency = [None, None]
        best_fit_positions = [None, None]
        adjusted_by_history = False
        R = 0
        if history is not None and len(history) > 0:

            history_rc = (np.array([h[1][0] for h in history]).mean(),
                np.array([h[1][1] for h in history]).mean())

            history_f = (np.array([h[2][0] for h in history]).mean(),
                np.array([h[2][1] for h in history]).mean())

        else:

            history_rc = None
            history_f = None

        #Obtaining current values
        for dimension in xrange(2):
            if median_coeff:

                positions[dimension], measures[dimension] = self.get_spikes(
                    dimension, im, visual, verbose, use_otsu, median_coeff,
                    manual_threshold=manual_threshold)

            else:

                positions[dimension], measures[dimension] = self.get_spikes(
                    dimension, im, visual, verbose, use_otsu,
                    manual_threshold=manual_threshold)

            #DEBUG ROBUSTNESS TEST
            #from random import randint
            #print "Positions before test:", len(positions[dimension])
            #pos_range = range(len(positions[dimension]))
            #for del_count in xrange(randint(1,5)+1):
                #del_pos = randint(0,len(pos_range)-1)
                #del pos_range[del_pos]
            #positions[dimension] = positions[dimension][pos_range]
            #measures[dimension] = measures[dimension][pos_range]
            #print "Deleted", del_count, "positions"
            #DEBUG END

            self.logger.info(
                "GRID ARRAY, Peak positions %sth dimension:\n%s" %\
                (str(dimension), str(positions[dimension])))

            best_fit_frequency[dimension] = r_signal.get_signal_frequency(\
                positions[dimension])

            if best_fit_frequency[dimension] is not None \
                                and history_f is not None:

                if abs(best_fit_frequency[dimension] /\
                            float(history_f[dimension]) - 1) > 0.1:

                    self.logger.warning(
                            ('GRID ARRAY, frequency abnormality for ' +\
                            'dimension {0} (Current {1}, Expected {2}'.format(
                            dimension, best_fit_frequency[dimension],
                            history_f)))

                    adjusted_by_history = True
                    best_fit_frequency[dimension] = history_f[dimension]

            best_fit_positions[dimension] = r_signal.get_true_signal(
                im.shape[int(dimension==0)], pinning_matrix[dimension],
                positions[dimension],
                frequency=best_fit_frequency[dimension],
                offset_buffer_fraction=0.5)

            if best_fit_positions[dimension] is not None and \
                                        history_rc is not None:

                goodness_of_signal = r_signal.get_position_of_spike(
                    best_fit_positions[dimension][0], history_rc[dimension],
                    history_f[dimension])

                if abs(goodness_of_signal) > 0.2:

                    self.logger.warning(("GRID ARRAY, dubious pinning " + \
                        "position for  dimension {0} (Current signal " + \
                        "start {1}, Expected {2} (error: {3}).".format(
                        dimension, best_fit_positions[dimension][0],
                        history_rc[dimension], goodness_of_signal)))

                    adjusted_by_history = True

                    new_fit = r_signal.move_signal(
                        [best_fit_positions[dimension]],
                        [-1 * round(goodness_of_signal)], freq_offset=0)

                    if new_fit is not None:

                        self.logger.warning(
                            "GRID ARRAY, remapped signal for " +\
                            "dimension {0} , new signal:\n{1}".format(
                            dimension, list(new_fit)))

                        best_fit_positions[dimension] = new_fit[0]

            ###START HERE MARKING OUT ALL OLD STUFF...
            #best_fit_start_pos[dimension], best_fit_frequency[dimension] = \
                #self.get_signal_position_and_frequency( measures[dimension],
                    #pinning_matrix[dimension], verbose)

            self.logger.info("GRID ARRAY, Best fit:\n" +\
                "* Elements: " + str(pinning_matrix[dimension]) +\
                "\n* Positions:\n" + str(best_fit_positions[dimension]))

            #DEBUGHACK
            #visual = True
            #DEBUGHACK - END

            if visual:
                Y = np.ones(pinning_matrix[dimension]) * 50
                Y2 = np.ones(positions[dimension].shape) * 100
                plt.clf()
                if dimension == 1:
                    plt.imshow(im[:,900:1200].T, cmap=plt.cm.gray)
                else:
                    plt.imshow(im[300:600,:], cmap=plt.cm.gray)

                plt.plot(positions[dimension], Y2, 'r*',
                    label='Detected spikes', lw=3, markersize=10)

                plt.plot(np.array(best_fit_positions[dimension]),\
                    Y ,'g*', label='Selected positions', lw=3, markersize=10)

                plt.legend(loc=0)
                plt.ylim(ymin=0, ymax=150)
                plt.show()
                #plt.savefig('signal_fit.png')
                #DEBUG HACK
                #visual = False
                #DEBUG HACK
            #if best_fit_start_pos[dimension] != None:

                #best_fit_positions[dimension] = \
                    #positions[dimension][best_fit_start_pos[dimension] : \
                        #best_fit_start_pos[dimension] + \
                        #pinning_matrix[dimension] ]

                #if visual:

                    #import matplotlib.pyplot as plt
                    #m_im = im.mean(axis=dimension)
                    #plt.plot(np.arange(len(m_im)), m_im, 'b-')
                    #Y = np.ones(len(best_fit_positions[dimension])) * 150
                    #plt.plot(np.array(best_fit_positions[dimension]),\
                        #Y ,'r*')

                #best_fit_positions[dimension] = \
                    #self.get_inserts_discards_extrapolations(\
                        #best_fit_positions[dimension],\
                        #best_fit_frequency[dimension],\
                        #pinning_matrix[dimension])

                #if visual:
                    #Y = np.ones(len(positions[dimension])) * 140
                    #plt.plot(np.array(positions[dimension]),\
                        #Y ,'g*') * 50
                    #Y = np.ones(len(best_fit_positions[dimension])) * 160
                    #plt.plot(np.array(best_fit_positions[dimension]),\
                        #Y ,'b*')
                    #plt.get_axes().set_ylim(ymin=-1,ymax=3)
                    #plt.show()

            if best_fit_positions[dimension] != None:

                #Comparing to previous
                if self.best_fit_positions != None:
                    if self.best_fit_positions[dimension] != None:
                        R += ((best_fit_positions[dimension] - \
                            self.best_fit_positions[dimension])**2).sum() / \
                            float(pinning_matrix[dimension])



                        #Updating previous
                        self.logger.info(
                            "GRID ARRAY, Got a grid R at, {0}".format(R))

        #DEBUG R
        #fs = open('debug_R.log','a')
        #if self.best_fit_positions is None:
            #fs.write(str([best_fit_positions[0][0],
                    #best_fit_positions[1][0]]) + "\n")
        #else:
            #fs.write(str([R, (best_fit_positions[0][0],
                #best_fit_positions[1][0]),
                #(self.best_fit_positions[0][0],
                #self.best_fit_positions[1][0]) ]) + "\n")
        #fs.close()
        #DEBUG END

        if R < 20 and best_fit_positions[0] != None and \
                             best_fit_positions[1] != None:

            #self.best_fit_start_pos = best_fit_start_pos
            self.best_fit_frequency = best_fit_frequency
            self.best_fit_positions = best_fit_positions
            self.R = R

        else:

            self.R = -1

        if self.best_fit_positions == None:

            return None, None, None, adjusted_by_history

        else:

            ret_tuple = (self.best_fit_positions[0],
                    self.best_fit_positions[1], self.R,
                    adjusted_by_history)

            return ret_tuple


"""
FROM: analysis_grid_cell_disection.py
CLASS: Blob
"""


    def set_first_step_filtering(self):
        """Crashed this on my way to toronto, fix!" """

        #DEBUG CODE START
        #from matplotlib import pyplot as plt
        #plt.subplot(221, title='Start image')
        #plt.imshow(self.grid_array)
        #DEBUG CODE END

        #De-noising the image with a smooth
        detect_im = gaussian_filter(self.grid_array, 2)

        #detect_im = median_filter(self.grid_array, size=(3, 3),
        #                mode="nearest")

        #DEBUG CODE START
        #plt.subplot(222, title = 'Image after gauss')
        #plt.imshow(self.grid_array)
        #plt.subplot(223, title = 'Filter after gauss')
        #plt.imshow(self.filter_array)
        #DEBUG CODE END

        #Threshold the image
        self.threshold_detect(im=detect_im)

        #DEBUG CODE START
        #plt.subplot(224, title = 'Filter after threshold')
        #plt.imshow(self.filter_array)
        #plt.show()
        #DDEBUG CODE END

        #self.filter_array = sobel(self.grid_array)

        #print np.sum(self.filter_array), "pixels inside at this stage"
        #from scipy.ndimage.filters import sobel
        #from scipy.ndimage.morpholgy import binary_erosion, binary_dilation,
        #    binary_fill_holes, binary_closing

        #Not neccesary per se, but we need a copy anyways
        #mat = cv.fromarray(self.filter_array)
        #print "**Mat made"
        #eroded_mat = cv.CreateMat(mat.rows, mat.cols, cv.CV_8UC1)

        #Erosion kernel
        #kernel = get_round_kernel(radius=2)
        #print kernel.astype(int)
        #print "***Erosion kernel ready"

        self.filter_array = binary_erosion(self.filter_array,
                                    structure=self.kernel)

        #Erode, radius 6, iterations = default (1)
        #kernel = cv.CreateStructuringElementEx(radius*2+1, radius*2+1,
        #    radius, radius, cv.CV_SHAPE_ELLIPSE)

        #print "Kernel in place", kernel
        #cv.Erode(mat, eroded_mat, kernel)
        #print "Eroded"
        #print np.sum(self.filter_array), "pixels inside at this stage"
        #Dilate, radius 4, iterations = default (1)
        #radius = 4
        #kernel = cv.CreateStructuringElementEx(radius*2+1, radius*2+1,
        #    radius, radius, cv.CV_SHAPE_ELLIPSE)

        #kernel = get_round_kernel(radius=4)

        #print "Kernel in place"
        self.filter_array = binary_dilation(self.filter_array,
                                        structure=self.kernel)

        #cv.Dilate(mat, mat, kernel)
        #print "Dilated"
        #print np.sum(self.filter_array), "pixels inside at this stage"

        #self.filter_array = binary_closing(self.filter_array)
                                    #structure=kernel)

        #print "Closing applied"
        #print np.sum(self.filter_array), "pixels inside at this stage"

        #self.filter_array = sobel(self.filter_array)

        #print "Edged detected"
        #print np.sum(self.filter_array), "pixels inside at this stage"

        #self.filter_array = binary_fill_holes(self.filter_array,
                    #structure=np.ones((5, 5)))

        #print "Holes filled"
        #print np.sum(self.filter_array), "pixels inside at this stage"

    def edge_detect_sobel(self):
        """This is a scetch for another detect, and should Not
        bw used"""

        from matplotlib import pyplot

        #De-noising the image with a smooth
        self.filter_array = gaussian_filter(self.grid_array, 2)
        pyplot.imshow(self.filter_array)
        pyplot.savefig('blob_gauss.png')
        pyplot.clf()

        #Checking the second dirivative
        #self.filter_array = laplace(self.filter_array)
        self.filter_array = sobel(self.filter_array, 0) ** 2 + \
                            sobel(self.filter_array, 1) ** 2

        pyplot.imshow(self.filter_array)
        #pyplot.savefig('blob_laplace.png')
        pyplot.savefig('blob_sobel.png')
        pyplot.clf()

        #self.filter_array = gaussian_filter(self.filter_array, 2)
        #pyplot.imshow(self.filter_array)
        #pyplot.savefig('blob_gauss2.png')
        #pyplot.clf()

        #Thesholding the edges
        self.threshold_detect(im=self.filter_array, color_logic='norm',
            threshold=np.max(self.filter_array) * 0.2)

        pyplot.imshow(self.filter_array)
        pyplot.savefig('blob_theshold.png')
        pyplot.clf()

        kernel = get_round_kernel(radius=3)
        self.filter_array = binary_dilation(self.filter_array,
                                            structure=kernel)

        pyplot.imshow(self.filter_array)
        pyplot.savefig('blob_dilation.png')
        pyplot.clf()

        kernel = get_round_kernel(radius=2)
        self.filter_array = binary_erosion(self.filter_array, structure=kernel)

        pyplot.imshow(self.filter_array)
        pyplot.savefig('blob_erosion.png')
        pyplot.clf()

        label_array, number_of_labels = label(self.filter_array)
        #print number_of_labels
        kernel = get_round_kernel(radius=1)
        center_size = 2
        circle_parts = []

        for i in xrange(number_of_labels):

            cur_item = (label_array == (i + 1))
            cur_pxs = np.sum(cur_item)

            if cur_pxs > 100:
                #c_o_m = center_of_mass(cur_item)
                #print "Mass centra: ", c_o_m
                oneD = np.where(np.sum(cur_item, 1) > 0)[0]
                dim1 = (oneD[0], oneD[-1])
                oneD = np.where(np.sum(cur_item, 0) > 0)[0]
                dim2 = (oneD[0], oneD[-1])
                cur_off = 2
                good_part = True

                if cur_item[dim1[0]: dim1[1], dim2[0] + cur_off].sum() / \
                                float(cur_pxs)\
                                >= cur_pxs / float(dim2[1] - dim2[0]):

                    good_part = False

                if good_part and cur_item[dim1[0]: dim1[1],
                                dim2[1] - cur_off].sum() / float(cur_pxs)\
                                >= cur_pxs / float(dim2[1] - dim2[0]):

                    good_part = False

                if cur_item[dim2[0]: dim2[1], dim1[0] + cur_off].sum() / \
                                float(cur_pxs)\
                                >= cur_pxs / float(dim1[1] - dim1[0]):

                    good_part = False

                if good_part and cur_item[dim2[0]: dim2[1],
                                dim1[1] - cur_off].sum() / float(cur_pxs)\
                                >= cur_pxs / float(dim1[1] - dim1[0]):

                    good_part = False

                #if cur_item[c_o_m[0]-center_size:c_o_m[0]+center_size,
                #    c_o_m[1]-center_size: c_o_m[1]+center_size].sum() > 0:

                if good_part == False:

                    pyplot.imshow(binary_erosion((label_array == (i + 1)),
                                                        structure=kernel))

                    pyplot.savefig('blob_item_' + str(i + 1) + '_bad.png')
                    pyplot.clf()

                else:

                    pyplot.imshow(binary_erosion((label_array == (i + 1)),
                                                        structure=kernel))

                    pyplot.savefig('blob_item_' + str(i + 1) + '.png')
                    pyplot.clf()
                    circle_parts.append(i + 1)

        self.filter_array = np.zeros(self.filter_array.shape)

        for c_part in circle_parts:

            #print self.filter_array.shape, label_array.shape
            self.filter_array += (label_array == c_part)

