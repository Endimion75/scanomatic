\documentclass{article}
%set spelllang=en_GB.UTF-8

\usepackage{times}
\usepackage{color}
\usepackage{parskip}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm, left=3.5cm, right=3.5cm]{geometry}

\definecolor{olive}{rgb}{0.1,0.5,0.05}

\newcommand{\comment}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{olive}{TODO: #1}}
\newcommand{\innovative}[1]{\emph{INNOVATIVE: #1}}

\title{Scan-o-Matic Analysis Procedure}
\author{Martin Zackrisson}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
\emph{This document outlines the procedures involved with scan-o-matic}

Scan-o-matic here refers not merely to the software, but to the entire work flow.
It consists of a planning phase that allows users to quickly compose 
experimental layouts according to what will be described below and to match up
results (that come as grid coordinates) with strain names.
The lab work described consists mainly of casting agar-based nutrient plates
and handling of a pinning robot (existing technology).
Thus, the wet lab aspect scripts a structure for laying out the experiments.
The experimental plates are placed in ordinary flat bed scanners, and 
remain there for the duration of the experiment.
The software controls the scanners and automatically acquires images of the 
growing colonies at regular intervals.
It also automatically locates the colonies and evaluates the number of cells
per colony via a number of transpositions of the pixel values according to
two distinct normalisation procedures.
The resulting colony growth curves are further analysed by extracting a
secondary characteristic, the generation time.
This feature, which comes from the early stages of growth has an
inherent property of being independent of neighbouring colonies growth.
A final spatial normalisation is performed to take away the occurrence of
positively neighbouring effects, which source is unknown at present.

In sum, the most important aspects are the work towards reducing the
complexity of the problems in combination with algorithms that reduce
the noise in the measures.

\end{abstract}

\section{Licences}

The development code is contained in a git (version handling program) 
repository on www.gitorious.org under GPLv3 licence and is hence publically
available and free to use.
Python code in itself is not distributed in compiled form, but as source.
Hence the code is in itself transparent.
The code implies the analytical algorithms, as they are part of the code,
and those are thus equally available and to some extent described in text
in the code base.
The code relies on several 3rd party modules all which have their own 
licenses.
The crash test dummy marking was inspired by an image found while googling,
but drawn by me using Inkscape (vector based image program).

\section{Pre-project/Planning}

Not explicitly part of Scan-o-Matic, Luciano's planner for keeping track of
what is were throughout the experiment and allow for fast and easy mapping of
strains to plate coordinate.

If planner is not in place, it is reasonable to assume that this can become a
bottle-neck and a major source of errors.

\comment{This segment is vital from the information processing aspect of 
the work flow. Without it, too much time would be spent trying to fit
experimental results with the individual experiment descriptions}

\todo{Planner needs to be operational}

\section{Wet Lab}

\subsection{RoToR}

In general everything should be assumed that it is scaled up to 1536-format for
final agar plates.
The last step should be a special 3:1 pinning of two different 384-plates (
One with experiments, that will become triplicates on the final plate.
One with a reference strain in all positions / as a lawn).
How the 384-plates were constructed does not matter.

\innovative{The pattern of 3:1 pinning work as local internal controls 
which solves spacial artefacts (described later) and function as inter
scanner normalisation as well.}

\subsection{Agar Plates}

\emph{Plate here does not refer to the properties of the plastic used,
but to what agar media casted in such plastic plates.}

Plates can be any sort for the preculture plates suitable to the strains used.
E.g. casted from YPD media.
But the experimental plates need to be as transparent as possible, hence
need to follow the BioScreen-media strategy.
Also, the experimental plates need to be done the same day as the 
experiment is initiated.

\innovative{Though pertaining to the whole layout of the work flow, the
experiments are run with 1536 colonies per plate, 4 plates per scanner
which results in new levels of high precision, high throughput screening
on agar.}

\todo{Test how dark media is permissible without loosing depth resolution
in colonies}

\section{Image acquisition}

Images are acquired every 20 minutes at default settings for three days.
Image settings are 600 dpi, 8bit, grayscale, TPU (Transparency Unit).
The plates with the colonies are placed lid-less inside the scanner at start
of the experiment and stays for the full experiment.
The plates are in a fixture, which has crash-test dummy features such that
the features (plates) can be triangulated (positioned using three known
points) from configuration settings.
The fixture further includes a Kodak Scanner Target that is used to normalise
brightness changes over the grayscale spectrum.
The software communicates with an energenie PowerManager to turn on power right
when the scanner is needed and off as soon as the image has been captured.
This makes the lamp have equal temperature at the start of each scan.

\innovative{Transparency give depth resolution for colonies. 
The transparency measure is also less sensitive to erroneous blob detection
(the border pixels of the colony or pixels outside the colony are likely to
have close to zero colony depth value compared to pixels that map to the 
interior of the colony).
The PowerManager makes all scans equal as the lamp will have approximately 
the same temperature each time.
The Kodak Scanner Target validates and works as fail-safe for the PowerManger
solution and normalise pixel values between scanners.
Fixture with crash-test dummies makes sectioning more reliable and its
automation easier}

\todo{Investigate the relapse behaviour of the new rig in the 30-degree room
that only scans a partial surface}

\section{First Pass Analysis}

Each image is individually analysed right after the image has been captured.
The images' features: plates and grayscale are triangulated.
The grayscale is analysed using low-level image analysis tools, primarily
edge detection and knowledge of expected grayscale segments.

\innovative{The combination of and specific settings of the 
image analysis algorithms are probably so, while all used are probably
quite expected. Same goes for all image analysis.}

\section{Second Pass Analysis}

The stack of images captured are jointly analysed traversing time backwards.
Each plate is treated independently (sectioned out using coordinates of 
the first pass analysis).
For each plate, a grid is placed using user supplemented information of the
number of rows and columns of colonies.
The placement of the grid is done per dimension (taking the mean of each
column or row of pixels) using the fact that the
colonies are darker than the non-colonies.
Since they are placed by a robot, they are in straight lines, producing
a signal that can be segmented, edge-detected and analysed for frequency
patterns that match the gridding specified by the user.
The relevant section of the image is transposed using a polynomial function
that estimates the solution to the detected grayscale values of the first
pass analysis and their target values supplied by Kodak.

\innovative{Backwards transversal.
Grid is place once, when it is easy and used for all images, this is 
possible dues to the fixture (which implies that the only mode by 
which the grid could be offset in relation to the colonies over time 
is if the agar moves inside the plate).
And as mentioned above the specifics
for the image analysis is probably somewhat unique though not fancy.
The use of the Kodak target has been mentioned.
The robustness of gridding procedure as a whole for the early images
when \emph{de novo} gridding would be a hard problem is probably
unique (it is also the early images that are the most interesting
for the resulting phenotypes).}

\subsection{Grid Cell Analysis}

For each intersection in the grid, a neighbouring area is sectioned out (called
grid cell).
The grid cell is segmented into blob and not-blob.
Noisy blob detection is avoided via a combination of pre-processing (smoothing) 
and size-selection (combination of erosion and dilation).
Finally if more than one blob is detected, it there are two non-connected,
pixel areas in the blob-segment, each is evaluated for their roundness and size.
The largest and roundest is considered the true blob.
The background defined as the not-blob area with a safety margin to the blob.
The background mean is subtracted from all pixels (or pixels are set to 0 if
they end up negative).

\innovative{To only look at the darkening effect of the cells}

\subsubsection{Inter-Scan Validation}

The detected blob's position and size is compared with the previous blob
to assert that it makes sense.
If new blob changes too drastically (heuristics), the previous blob is kept
and the new blob-detection is discarded.
In this case the cell count calibration and measures are hence done on the 
current image using the blob detected in the previous image.
Rational being that the detection becomes more difficult as colonies get 
fainter as analysis approaches experiment start.

\subsubsection{Cell Count Calibration}

Separate drop test experiment using the software in manual mode (no automatic
plate sectioning and gridding), have been performed.
The results of the drop test were scanned, then cut out, cells vortexed off
the agar, and amount estimated by $OD_{600}$.
The resulting estimate of cell numbers was paired with the vector of all
pixels in the matching blob after background had been subtracted.
This was done for 50(?) colonies with different starting dilutions and after
different time spent growing on the plate.
A polynomial was constructed such that the sum of applying it to each element
of the pixel-vector equalled the reference cell number estimate.

This polynomial is used transposing the grid cell's pixels (which have 
already been transposed once) a final time such that they estimate the 
true number of cells in each pixel.

\innovative{The combined wet lab and image analysis method for finding 
a polynomial that describes how a number of X cells in a part of the 
colony cause a decrease in pixel brightness Y, for the pixel p(x,y) 
is most probably new.}

\todo{Incorporate the ability of calibration measures directly into the 
software}

\subsubsection{Measures}

Basic properties such as area (number of pixels) and pixelsum are acquired.
The latter the one used for further analysis.

\subsection{Output Format}

Data is stored in XML-format

\subsection{Phenotype calculation}

Precog is used to extract phenotypes. Those are saved out as CSV:s
This is all that Precog is currently used for.

\comment{If Phenotype post-processing is not included in Precog, then
I don't see the purpose of Precog -- that is it should be stripped from
the pipeline or greatly extended}

\innovative{Using the BioScreen algorithms of growth rate phenotype 
extraction from growth curves on agar screens is probably new.
As a consequence, measures are taken early, such that there are no
negative neighbouring effect nor edge effects.}

\subsection{Phenotype post-processing and Growth Curve Evaluation}

A supplementary command-line script (that comes with scan-o-matic),
is used to combine the phenotype-CSV and the growth curve-XML.
The script assists in removing failed experiments or position that were
in fact blank, by a combination of automatic, semi-automatic and manual
interfaces.
When only trusted phenotypes remain, the references strain positions
(that came from the 3:1 pinning) are used to create a normalisation
surface over the full plate via interpolation.
This surface is subtracted from all positions and afterwards the 
mean of the surface is added back.
This takes away the positional effects.

\innovative{As mentioned, the normalisation surface is new.
To some extent the assisted growth curve evaluation is new.}

\subsection{Output Format 2}

The results of phenotype post-processing can be saved as NumPy
arrays or CSV-files.

\end{document}
