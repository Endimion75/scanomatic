<!DOCTYPE html>
<html>
<head>
	<link rel="stylesheet" type="text/css" href="style.css">
	<title>Scan-o-Matic</title>
	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
</head>
<body>
<div id='toc'>
	<h1>Contents</h1>
	<h2><a href="#installing">Installation</a></h2>
	<h2><a href="#files">The Files</a></h2>
	<h2><a href="#conversion">XML Conversion</a></h2>
	<h2><a href="#extraction">Feature Extraction</a></h2>
	<h2><a href="#quality">Quality Control</a></h2>
	<h2><a href="#server">The Server</a></h2>
</div>
<img id='logo' src='images/help_logo.png'>
	<script>
$("#logo").bind("load", function () { $(this).hide().fadeIn(4000); });
	</script>
<div id='cont'>
	<div class='section'>
		<h1><a id="installing">Installation</a></h1>
		<p><em>Please note that each Scan-o-Matic update downloaded must
			be installed using the procedure described here.</em></p>
		<p>Scan-o-Matic can is easiest installed by running
		<code>python setup.py install --user</code>. If you require it for
		all user, you may want to run <code>sudo python setup.py install</code>
		instead.
		Whichever method you choose, to avoid headaches down the road,
		stick to one of them.
		</p>
		<p>The setup may ask you to install other non-python dependencies,
		these are not needed if the install is not intended to control
		any scanners.
		In such a case you can simply dismiss that dialog.</p>
		<p>Scan-o-Matic uses several python modules and the setup script will
		attempt to install them. If however this fails the following should
		install the required modules:</p>
		<blockquote><code>sudo easy_install argparse matplotlib 
		multiprocessing odfpy numpy sh configparse scikit-image uuid PIL scipy
		pygtk</code></blockquote>
		<p>The module <code>nmap</code> may need special install routines.
		On Ubuntu systems it is easily done with
		<code>sudo apt-get install nmap python-nmap</code>,
		else you may have to build it:
		<a class="linkOut" href="http://xael.org/norman/python/python-nmap/">More info here</a>.
		</p>
		<h2>Launching Scan-o-Matic</h2>
		<p>After the install several Scan-o-Matic scripts will be accessible
		within the system.
		All of them start with <code>scan-o-matic</code>.
		The main program is fittingly the one called exactly so.
		Should there be a problem reaching the launch scripts, then this
		is probably due to a user install where it happens that python and
		the OS disagree where to place them.
		The setup script should have prompted you with instructions on how
		to fix this.</p>
		<h2>Removing Installed Files</h2>
		<p>User data and application configurations and several log-files
		are all stored in a directory called <code>.scan-o-matic</code>
		directly in the user's home directory.</p>
		<p>The location of the python files and the launch scripts will
		depend on your OS and the way you installed them.
		When the <code>setup.py</code> script is run, it typically will inform
		on where the files were put and these can simply be removed manually.
		</p>
		<p>On Ubuntu the scanomatic-python library is put in either
		<code>~/.local/lib/python2.7/site-packages/scanomatic/</code>
		or <code>/usr/lib/python2.7/dist-packages/scanomatic/</code>
		depending on install-method.
		The scanomatic-folder can be removed entirely.</p>
		<p>To remove the launch-scripts <code>rm scan-o-matic*</code> in the
		appropriate folder should be safe.
		Launch scripts are either put in <code>/usr/bin</code> or
		<code>~/.local/bin/</code>.
		</p>

	</div>
	<div class='section'>
		<h1><a id="files">The Project Files</a></h1>
		<p>Here follows a description of most of the files encountered
		within a project, their use and value.</p>
		<p>Each project is housed in its own directory.
		The project's base directory holds the primary data, which is the
		image files and the instructions given and used by scan-o-matic while
		aquiring those images.
		The project also holds one or more analysis folders, which holds the
		output of the image analysis, the growth curve feature extraction
		as well and final data completed by the quality and normalization
		step.
		</p>
		<h2>Image Files</h2>
		<p><u>File Pattern:</u> <code>PROJECTNAME_INDEX_TIME.tiff</code></p>
		<p><em>The image files contain complete information to rebuild
			all steps of growth data.</em>
		The only thing lacking is meta-data about what were the strains
		and the media.</p>
		<p>As an example, a file could be named
		<code>Clot_0185_222151.9179.tiff</code>,
		which means it is part of the project "Clot"
		and that it is the 185th scan after the initial zero-position.
		The scan was performed 222151.9179 seconds after the project was
		started.</p>
		<h2>First Pass Analysis</h2>
		<p><u>File Pattern:</u> <code>PROJECTNAME_1.pass.analysis</code></p>
		<p>This file holds the first pass image analysis results of the project
		as well as basic meta-data about the project.
		For practical purposes, even if image analysis is to be redone, life
		is much easier saving this file.</p>
		<p>The first-pass analysis primarily identifies the grayscale
		calibration values and the positions of the plates.</p>
		<h2>Fixture Config</h2>
		<p><u>File Pattern:</u> <code>fixture.config</code></p>
		<p>This is a copy of the scanner's fixture as it was configured
		when the project was initially started.</p>
		<h2>Experiment Run Log</h2>
		<p><u>File Pattern:</u> <code>experiment.run</code></p>
		<p>This is a log file for the experiment while it was running</p>
		<h2>Analysis: Image Analysis Files</h2>
		<p><u>File Pattern:</u> <code>analysis/image_INDEX_data.npy</code> and
		<code>analysis/time_data.npy</code></p>
		<p>The output of the image analysis is stored in numpy-format.
		There's one file for each image scanned and one seperate holding
		the vector of scan-times.
		Should specific scans be corrupt and needing removal, the corresponding
		numpy-file can simply be deleted.</p>
		<p>For compatibility with external applications there are also two
		xml-files created (<code>analysis/analysis.xml</code> &amp; 
		<code>analysis/analysis_slimmed.xml</code>).
		Neither is used by Scan-o-Matic, but should your project lack any or
		all of the image analysis files, they can be
		<a href="#conversion">recreated from these xml-files</a>.</p>
		<h2>Analysis: Feature Extraction</h2>
		<p><u>File Pattern:</u> <code>analysis/phenotypes_raw.npy</code>,
		<code>analysis/curves_raw.npy</code>,
		<code>analysis/curves_smooth.npy</code>
		<code>analysis/phenotype_params.npy</code> and 
		<code>analysis/phenotype_times.npy</code></p>
		<p>The curve files represent the same data as the image analysis data
		files but restructured by each colony, instead of by scans.
		The smooth file also has been processed according to the smoothing
		algorithm for curves.</p>
		<p>The phenotype_params.npy contains the feature extraction settings
		used.
		The phenotype_times.npy has the data-times used.
		The phenotypes_raw.npy holds the un-normalized features extracted
		from the crues.</p>
		<h2>Analysis: Quality Control</h2>
		<p><u>File Pattern:</u> <code>analysis/phenotypes_filter.npy</code></p>
		<p>This file holds information about what curves have been removed.
		If this file is deleted, all removed curves will reappear.</p>
		<p>When saving the state in the quality control interface, it is
		this filter file that is updated.</p>
		<h2>Analysis: Normaliztion</h2>
		<p>There are no internal files represented by this stage, instead
		it is up to the user to save the final output as the user sees fit.</p>
	</div>
	<div class='section'>
		<h1><a id="conversion">XML Conversion</a></h1>
		<p>If a project lacks a native image analysis output, but has a
		slimmed version of an XML output, then it is possible to convert.
		The conversion is run as a separate process, and not handled by the
		server.
		For this reason, it is not good to do too many at the same time as
		they may eat up all available resources on the system.
		This is particularly important if the same computer is used to
		run experiments.</p>
		<p>There is a live status that doesn't say how long time each
		conversion is going to take, but reports when they are done.</p>
	</div>
	<div class='section'>
		<h1><a id="extraction">Feature Extraction</a></h1>
		<p>To be able to run feature extraction on a project, the
		image analysis output must be in the native numpy format.
		If this isn't the case, then do an XML-conversion (see above).</p>
		<p>The extraction is rather straight forward and is run by the server.
		It will overwrite any previous feature extractions and quality
		control done on such extraction, should that exist in the
		designated folder.</p>
	</div>
	<div class='section'>
		<h1><a id="quality">Quality Control</a></h1>
		<h2>Workflow</h2>
		<p>The general workflow is as follows:</p>
		<blockquote>
		<em>Load Data</em> &gt; <em>Load Meta Data</em> (<u>Optional</u>) &gt;
		<em>Quality control positions on all plates</em> &gt; 
		<em>Set reference positions</em> &gt; <em>Normalization</em> &gt;
		<em>Saving</em>.
		</blockquote>
		<p>
		If meta-data is loaded, the output of saving will automatically add
		the phenotype information after the metadata for each strain.
		More on the requirements on the metadata further down.
		The quality control is assisted by the system ranking the positions
		on how good/bad they seem.
		<em>Note</em> that this ranking is done on per phenotype.
		Positions can either be selected by clicking on the heatmap or
		by browsing them on their ranks.
		When a position is selected, its corresponding curve is shown on
		the curves panel.
		If no data exists for the position, the curves panel will appear
		empty.
		To be able to normalize, the reference positions must be set.
		Which is the correct offset is usually easy to spot due to its
		relatively systematic deviation from other positions.
		When normailisation has been done, new phenotypes appear in the
		drop-down selector.
		</p>

		<h2>Meta Data</h2>
		<p>The meta data is only read from ods (Open Office Spreadsheets).
		The metadata is expected to be organized as the plates are.
		First and foremost each meta-data sheet in the selected meta-data
		files must come in the order of the plates.
		Each sheet used must have information for either a whole plate or
		for one of the subplates.
		Each sheet can either have exactly as many rows needed or one more,
		in which case it will be treated as a header row and not used.
		The order always follows the long axis as first axis.

		<h2>Quality Control</h2>

		<h3>Hotkeys</h3>
		<ul class="keylist">
			<li><code>1</code> - <code>9</code>: Selects the corresponding plate</li>
			<li><code>D</code>: Removes curve for all phenotypes</li>
			<li><code>P</code>: Toggles selection of new phenotype</li>
			<li><code>B</code>, <code>K</code>: Selects curve with higher rank</li>
			<li><code>N</code>, <code>J</code>: Selects curve with lower rank</li>
			<li><code>U</code>: Undo</li>
		</ul>

		<p>
		To get a proper normalisation, bad curves must be removed.
		The evaluation of curve quality should primarily be done by checking
		if <b>(A)</b> there's high level on noise, <b>(B)</b> there's uncommon
		types of noise that weakens the general trust in the curve and
		<b>(C)</b> the phenotype of interest seems to be affected by
		<em>normal</em> noise located precisely so that it affects the
		phenotype.
		</p>

		<h2>Normalization</h2>

		<p>By default usage, standard parameters are used to normalize.
		There's a possibility to test other parameter settings by starting
		the quality control interface as a stand-alone application from the
		command line by running <code>scan-o-matic_qc -d</code>.</p>
		<p>When normalization has been performed new phenotypes appear in the
		selector</p>

		<h2>Saving</h2>

		<p>Saving original data will save the phenotypes as they were read
		into a csv type format but with one exception.
		<em>Removed curves will be written as missing data</em></p>

		<p>Saving normalized saves only those phenotypes that are normalized.
		Put differently, it saves the information on the phenotypes that appear
		after the normalization button has been pressed.
		</p>

		<p>Saving the state saves the internal representation on what has been
		removed.
		By doing so, one can easily pick up work later from exactly the same
		position as one was.
		The state remembers undo.</p>
	</div>
	<div class='section'>
		<h1><a id="server">The Server</a></h1>
		<p>The purpose of the server is to work in the background with
		tasks that last over more than neglectable time.
		It also correlates the workload of the computer so that it doesn't
		stall as a result of eager scientists.
		The server is automatically started as soon as Scan-o-Matic starts
		and will remain running indefinately until the computer is shut down.
		</p>
		<p>Should the need arise to force a stop and/or restart of the
		server, such controls can be accessed inside Scan-o-Matic under
		<u>Application Config</u></p>
	</div>
</div>
</body>
</html>
